{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93bd8094",
   "metadata": {},
   "source": [
    "## Importing libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffc7c5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "SPEAKER_INFO_FILE_PATH = os.path.join(DATA_DIR, 'speaker_attributes.parquet')\n",
    "CACHE_FILE_PATH = os.path.join(CACHE_DIR, 'processed_data.json.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da60e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import bz2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 1.471 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 3.039 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 4.674 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 6.211 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 7.806 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 9.312 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 10.867 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2015.json.bz2 in 12.362 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2015.json.bz2 in 14.016 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2015.json.bz2 in 15.655 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2015.json.bz2 in 17.365 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2015.json.bz2 in 19.032 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2015.json.bz2 in 20.582 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2015.json.bz2 in 22.231 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2015.json.bz2 in 23.799 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2015.json.bz2 in 25.331 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2015.json.bz2 in 26.879 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2015.json.bz2 in 28.437 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2015.json.bz2 in 29.666 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2015.json.bz2 in 30.396 minutes\n",
      "Finished processing Data\\quotes-2015.json.bz2 in 30.984 minutes\n",
      "Starting processing Data\\quotes-2016.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2016.json.bz2 in 0.733 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2016.json.bz2 in 1.447 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2016.json.bz2 in 2.143 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2016.json.bz2 in 2.877 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2016.json.bz2 in 3.573 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2016.json.bz2 in 4.302 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2016.json.bz2 in 5.010 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2016.json.bz2 in 5.687 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2016.json.bz2 in 6.378 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2016.json.bz2 in 7.049 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2016.json.bz2 in 7.787 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2016.json.bz2 in 8.484 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2016.json.bz2 in 9.163 minutes\n",
      "Finished processing Data\\quotes-2016.json.bz2 in 9.749 minutes\n",
      "Starting processing Data\\quotes-2017.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2017.json.bz2 in 0.792 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2017.json.bz2 in 1.538 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2017.json.bz2 in 2.290 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2017.json.bz2 in 3.055 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2017.json.bz2 in 3.817 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2017.json.bz2 in 4.564 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2017.json.bz2 in 5.285 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2017.json.bz2 in 6.030 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2017.json.bz2 in 6.767 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2017.json.bz2 in 7.531 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2017.json.bz2 in 8.252 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2017.json.bz2 in 8.990 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2017.json.bz2 in 9.714 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2017.json.bz2 in 10.459 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2017.json.bz2 in 11.226 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2017.json.bz2 in 11.982 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2017.json.bz2 in 12.704 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2017.json.bz2 in 13.447 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2017.json.bz2 in 14.194 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2017.json.bz2 in 14.936 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2017.json.bz2 in 15.726 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2017.json.bz2 in 16.475 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2017.json.bz2 in 17.261 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2017.json.bz2 in 18.019 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2017.json.bz2 in 18.760 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2017.json.bz2 in 19.498 minutes\n",
      "Finished processing Data\\quotes-2017.json.bz2 in 20.061 minutes\n",
      "Starting processing Data\\quotes-2018.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2018.json.bz2 in 0.806 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2018.json.bz2 in 1.622 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2018.json.bz2 in 2.362 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2018.json.bz2 in 3.115 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2018.json.bz2 in 3.833 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2018.json.bz2 in 4.557 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2018.json.bz2 in 5.267 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2018.json.bz2 in 5.990 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2018.json.bz2 in 6.990 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2018.json.bz2 in 7.753 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2018.json.bz2 in 8.487 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2018.json.bz2 in 9.259 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2018.json.bz2 in 9.987 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2018.json.bz2 in 10.718 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2018.json.bz2 in 11.499 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2018.json.bz2 in 12.277 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2018.json.bz2 in 13.483 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2018.json.bz2 in 14.262 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2018.json.bz2 in 15.003 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2018.json.bz2 in 15.920 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2018.json.bz2 in 17.075 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2018.json.bz2 in 17.972 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2018.json.bz2 in 18.856 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2018.json.bz2 in 19.815 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2018.json.bz2 in 20.731 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2018.json.bz2 in 21.819 minutes\n",
      "Processed 27000000 lines from Data\\quotes-2018.json.bz2 in 23.510 minutes\n",
      "Finished processing Data\\quotes-2018.json.bz2 in 23.735 minutes\n",
      "Starting processing Data\\quotes-2019.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2019.json.bz2 in 0.913 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2019.json.bz2 in 1.773 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2019.json.bz2 in 2.668 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2019.json.bz2 in 3.913 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2019.json.bz2 in 4.847 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2019.json.bz2 in 5.756 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2019.json.bz2 in 6.696 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2019.json.bz2 in 7.578 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2019.json.bz2 in 8.764 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2019.json.bz2 in 10.225 minutes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# /!\\ TfidfVectorizer can't be batched! If appropriate, use CountVectorizer + TfidfTransformer instead\n",
    "# (equivalent, but CountVectorizer can be batched).\n",
    "@utils.cache_to_file_pickle(\"function-embed_sentences\")\n",
    "def embed_sentences(data_dir):\n",
    "    count_vectorizer = TfidfVectorizer(strip_accents = \"ascii\", lowercase = True, stop_words = \"english\",\n",
    "                                       norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = False)\n",
    "    \n",
    "    matrix = count_vectorizer.fit_transform(line['quotation'] for line in all_quotes_generator(data_dir))\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "matrix = embed_sentences(data_dir = DATA_DIR)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "TruncatedSVD(n_components = 100, n_iter = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
