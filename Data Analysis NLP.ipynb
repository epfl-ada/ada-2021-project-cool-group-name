{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93bd8094",
   "metadata": {},
   "source": [
    "## Importing libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffc7c5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "SPEAKER_INFO_FILE_PATH = os.path.join(DATA_DIR, 'speaker_attributes.parquet')\n",
    "CACHE_FILE_PATH = os.path.join(CACHE_DIR, 'processed_data.json.bz2')\n",
    "BERT_MODEL_SAVE_PATH = os.path.join(CACHE_DIR, 'bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dd420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(string):\n",
    "    # Make all lower-case.\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers.\n",
    "    string = re.sub(r'[^a-z ]', ' ', string)\n",
    "    \n",
    "    # Split into words.\n",
    "    words = string.split()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cf15e",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fabb99",
   "metadata": {},
   "source": [
    "**Note:** On long term, we may want to do two passes on the dataset: one to fit models and one to predict, as that should be less RAM intensive. Also, max_features, max_df and min_df in CountVectorizer may be tuned for same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df819501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "@utils.cache_to_file_pickle(\"function-count_matrix_dataset\")\n",
    "def count_matrix_dataset(data_dir):\n",
    "    count_vectorizer = CountVectorizer(strip_accents = \"ascii\", lowercase = True, stop_words = \"english\")\n",
    "    \n",
    "    counts_matrix = count_vectorizer.fit_transform(line['quotation'] for line in utils.all_quotes_generator(data_dir))\n",
    "    \n",
    "    return count_vectorizer, counts_matrix\n",
    "\n",
    "\n",
    "count_vectorizer, counts_matrix = count_matrix_dataset(data_dir = DATA_DIR)\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "del count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531d686",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dfba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def latent_semantic_analysis(counts_matrix):\n",
    "    tfidf_embedding = TfidfTransformer(norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = False).fit_transform(counts_matrix)\n",
    "\n",
    "    lsa_model = TruncatedSVD(n_components = 100, n_iter = 10)\n",
    "    lsa_model.fit(tfidf_embedding)\n",
    "    \n",
    "    return lsa_model\n",
    "\n",
    "lsa_model = latent_semantic_analysis(counts_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "going bay green away time action people think litigation resort\n",
      "Topic 1:\n",
      "think actually dad saw just played opportunity make great sure\n",
      "Topic 2:\n",
      "dad saw opportunity great words coming greatest movement came fair\n",
      "Topic 3:\n",
      "just sure play ve ready make played healthy minutes playing\n",
      "Topic 4:\n",
      "people able come want help committee members run fair carry\n",
      "Topic 5:\n",
      "long tail crocodile ll crack donald engineering president storm trump\n",
      "Topic 6:\n",
      "good long tail year crocodile high day performance want plays\n",
      "Topic 7:\n",
      "didn program cost feel team year want fit portion relationship\n",
      "Topic 8:\n",
      "want able ll does cat leash make season wanted deal\n",
      "Topic 9:\n",
      "doesn agent hoping getting does 94 football miller probably quickest\n",
      "Topic 10:\n",
      "new offer gas capabilities computer design fiction science tv watching\n",
      "Topic 11:\n",
      "high numbers good really glass does 365 cause damage degrees\n",
      "Topic 12:\n",
      "high far years numbers good didn want season new think\n",
      "Topic 13:\n",
      "road trade said day years came took home issue iffa\n",
      "Topic 14:\n",
      "tail like term came crocodile long cut workers american said\n",
      "Topic 15:\n",
      "issues did gas opposed waste code complaint drop hud insisted\n",
      "Topic 16:\n",
      "workers cut loan 21st act century tax high little ll\n",
      "Topic 17:\n",
      "action issues able network sense like higher gas systems power\n",
      "Topic 18:\n",
      "months goals job 18 32 ceo david ffa gallop kept\n",
      "Topic 19:\n",
      "like years paper printed salary high far past numbers make\n",
      "Topic 20:\n",
      "years play bowl super 11 winning ahead credit improve profile\n",
      "Topic 21:\n",
      "ll winning result loan law market cut industry little access\n",
      "Topic 22:\n",
      "power people higher good make plays wasn performance feel program\n",
      "Topic 23:\n",
      "provide cut higher loan season encore issue young 21st act\n",
      "Topic 24:\n",
      "provide decide greater interesting necessary penalties review issue communications enquiries\n",
      "Topic 25:\n",
      "away come points unbelievable fulfill law obviously systems sense potential\n",
      "Topic 26:\n",
      "iffa sense little trade 2016 like action team high numbers\n",
      "Topic 27:\n",
      "government functionaries lands parties rules sold underhand violation agreements allows\n",
      "Topic 28:\n",
      "potential don did committee used house athletics commitments donors list\n",
      "Topic 29:\n",
      "gas goals help attain business indian karma leaders values potential\n",
      "Topic 30:\n",
      "higher high numbers returns used market committee house chair commons\n",
      "Topic 31:\n",
      "potential loan athletics commitments donors list major point prospects stream\n",
      "Topic 32:\n",
      "like paper printed salary loan offer little dad words did\n",
      "Topic 33:\n",
      "sure 1000 miles numbers high power ready 100 available shelf\n",
      "Topic 34:\n",
      "region ve higher obviously aligning approach buffalo tried favorite king\n",
      "Topic 35:\n",
      "2016 hopefuls sotu twitter words dad away encore power absolute\n",
      "Topic 36:\n",
      "actual club haven moment received turin offer culture customs defense\n",
      "Topic 37:\n",
      "county poor hopefuls sotu twitter 2016 record agencies dot fault\n",
      "Topic 38:\n",
      "county poor encore numbers high agencies dot fault keeping management\n",
      "Topic 39:\n",
      "granny leader mature scott avoid crisis economists failed hubris predict\n",
      "Topic 40:\n",
      "granny avoid crisis economists failed hubris predict 1000 miles journalism\n",
      "Topic 41:\n",
      "granny leader mature scott journalism 1000 miles sure brings contrasts\n",
      "Topic 42:\n",
      "200 palm springs trip granny buildings corporations dollars harter land\n",
      "Topic 43:\n",
      "granny boyfriend journalism 100 available shelf ready brings contrasts drives\n",
      "Topic 44:\n",
      "boyfriend fortunate playmaker 200 palm springs trip avoid crisis economists\n",
      "Topic 45:\n",
      "iexcl los queremos vivos granny 2005 affected bombings foundations july\n",
      "Topic 46:\n",
      "leader mature scott drug generic mr needed price promise proof\n",
      "Topic 47:\n",
      "200 palm springs trip boyfriend 100 available shelf journalism ready\n",
      "Topic 48:\n",
      "fortunate playmaker 2005 affected bombings foundations july liberals london moral\n",
      "Topic 49:\n",
      "granny fortunate playmaker iexcl los queremos vivos 200 palm springs\n",
      "Topic 50:\n",
      "journalism fortunate playmaker 2005 affected bombings foundations july liberals london\n",
      "Topic 51:\n",
      "boyfriend leader mature scott granny 1000 miles buildings corporations dollars\n",
      "Topic 52:\n",
      "county poor fiction science tv watching week new cat leash\n",
      "Topic 53:\n",
      "encore help lead line machine type attain business indian karma\n",
      "Topic 54:\n",
      "encore hopefuls sotu twitter auction gifts holiday items perfect 2016\n",
      "Topic 55:\n",
      "hopefuls sotu twitter 2016 house chair commons effect health impressive\n",
      "Topic 56:\n",
      "away fulfill arrival benefit category combination completeness designed economy flexibility\n",
      "Topic 57:\n",
      "time anees security action litigation resort encore words gas amy\n",
      "Topic 58:\n",
      "culture customs defense effort freedom hungarians saw coming greatest movement\n",
      "Topic 59:\n",
      "try accept ministers perform prime probation way wood cut gas\n",
      "Topic 60:\n",
      "higher anees security try agent hoping returns actor blasts close\n",
      "Topic 61:\n",
      "favorite king programs try culture customs defense effort freedom hungarians\n",
      "Topic 62:\n",
      "away points unbelievable amy forced hours inadequate placed pressure staffing\n",
      "Topic 63:\n",
      "try 2015 ale brown heard lager ahead credit improve profile\n",
      "Topic 64:\n",
      "favorite king programs higher numbers agreements road high ahead credit\n",
      "Topic 65:\n",
      "home absolute compulsory encryption granted ico meantime minimum police accept\n",
      "Topic 66:\n",
      "functionaries lands parties rules sold underhand violation growing steven based\n",
      "Topic 67:\n",
      "favorite king programs growing steven based far amy forced hours\n",
      "Topic 68:\n",
      "try encore ahead credit improve profile times tough advantage bite\n",
      "Topic 69:\n",
      "higher functionaries lands parties rules sold underhand violation 2015 ale\n",
      "Topic 70:\n",
      "higher decide greater interesting necessary penalties review issue ahead credit\n",
      "Topic 71:\n",
      "agent hoping gas favorite king programs culture customs defense effort\n",
      "Topic 72:\n",
      "try advantage bite guy leverage decide greater interesting necessary penalties\n",
      "Topic 73:\n",
      "300 90 congested mainly percent problems running sites supply network\n",
      "Topic 74:\n",
      "paper printed salary athletes backward graciously hosts look modern vibrant\n",
      "Topic 75:\n",
      "2015 ale brown heard lager built ceiling certification current data\n",
      "Topic 76:\n",
      "iffa lead line machine type bars criminal having impact justice\n",
      "Topic 77:\n",
      "loan try controlled pboc quota rrr underwriting 25 estimate modernization\n",
      "Topic 78:\n",
      "gas arsenal beat cup drew fa league old trafford united\n",
      "Topic 79:\n",
      "numbers home high getting paper printed salary broken got realtor\n",
      "Topic 80:\n",
      "added booklet braille caught cd hidden inside sketch title paper\n",
      "Topic 81:\n",
      "home broken got realtor showed state town able encore 21st\n",
      "Topic 82:\n",
      "bowl super try road cat leash cost took winning able\n",
      "Topic 83:\n",
      "numbers bowl super term agent hoping opposed waste loan high\n",
      "Topic 84:\n",
      "94 football miller probably quickest step ware iffa trade given\n",
      "Topic 85:\n",
      "term crocodile numbers capabilities computer design played agent hoping ve\n",
      "Topic 86:\n",
      "capabilities computer design agent hoping offer higher chance defamation hitting\n",
      "Topic 87:\n",
      "capabilities computer design given whack getting road bars criminal having\n",
      "Topic 88:\n",
      "getting bars criminal having impact justice number rearrange term code\n",
      "Topic 89:\n",
      "crocodile opposed waste tail long capabilities computer design added booklet\n"
     ]
    }
   ],
   "source": [
    "def print_main_topic_words(feature_names, components, n_main_words = 10):\n",
    "    for i, component in enumerate(components):\n",
    "        words_making_up_component = {feature: coordinate for feature, coordinate in zip(feature_names, component)}\n",
    "        main_words = sorted(words_making_up_component, key = words_making_up_component.get, reverse = True)[:n_main_words]\n",
    "        print(f\"Topic {i}:\")\n",
    "        print(' '.join(main_words))\n",
    "        \n",
    "print_main_topic_words(feature_names, lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b98a1",
   "metadata": {},
   "source": [
    "#### LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66545678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 100, learning_method = 'batch', evaluate_every = -1, njobs = -1)\n",
    "lda_model.fit(counts_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_main_topic_words(feature_names, lda_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360baf44",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rajmehra03/topic-modelling-using-lda-and-lsa-in-sklearn\n",
    "\n",
    "https://yanlinc.medium.com/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae032de",
   "metadata": {},
   "source": [
    "### BERT Pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5d66f",
   "metadata": {},
   "source": [
    "https://github.com/MaartenGr/BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682d50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "@utils.cache_to_file_pickle(\"function-embed_dataset_BERT\")\n",
    "def embed_dataset_BERT(data_dir):\n",
    "    \n",
    "    # Min topics size should be fine-tuned: we have very large dataset hence a value of ~1000 is better than default one.\n",
    "    # Also, may want to play with nr_topics: either leave it to default none, or auto, or set a manual value large enough\n",
    "    # that topics do not get mixed up while merging. Also note it is heavy to run anything other than None.\n",
    "    # In background, BERTopic uses UMAP, HDBSCAN, CountVectorizer. Each has their set of parameters, of which only a\n",
    "    # subset is tunable via the BERTopic constructor. For best tuning, may want to look into them.\n",
    "    bert_model = BERTopic(embedding_model = \"all-MiniLM-L6-v2\", min_topic_size = 500)\n",
    "    \n",
    "    topics, probabilities = bert_model.fit_transform(utils.all_quotes_generator(data_dir, 'quotation'))\n",
    "    return bert_model, (topics, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2859b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 0.461 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 0.914 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 1.415 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 1.892 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 2.347 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 2.800 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 3.262 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2015.json.bz2 in 3.724 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2015.json.bz2 in 4.214 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2015.json.bz2 in 4.701 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2015.json.bz2 in 5.179 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2015.json.bz2 in 5.636 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2015.json.bz2 in 6.113 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2015.json.bz2 in 6.570 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2015.json.bz2 in 7.026 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2015.json.bz2 in 7.498 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2015.json.bz2 in 7.957 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2015.json.bz2 in 8.423 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2015.json.bz2 in 8.881 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2015.json.bz2 in 9.337 minutes\n",
      "Finished processing Data\\quotes-2015.json.bz2 in 9.747 minutes\n",
      "Starting processing Data\\quotes-2016.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2016.json.bz2 in 0.474 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2016.json.bz2 in 0.944 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2016.json.bz2 in 1.412 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2016.json.bz2 in 1.889 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2016.json.bz2 in 2.379 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2016.json.bz2 in 2.848 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2016.json.bz2 in 3.346 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2016.json.bz2 in 3.829 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2016.json.bz2 in 4.283 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2016.json.bz2 in 4.753 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2016.json.bz2 in 5.225 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2016.json.bz2 in 5.682 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2016.json.bz2 in 6.140 minutes\n",
      "Finished processing Data\\quotes-2016.json.bz2 in 6.543 minutes\n",
      "Starting processing Data\\quotes-2017.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2017.json.bz2 in 0.581 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2017.json.bz2 in 1.162 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2017.json.bz2 in 1.716 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2017.json.bz2 in 2.254 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2017.json.bz2 in 2.873 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2017.json.bz2 in 3.441 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2017.json.bz2 in 3.984 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2017.json.bz2 in 4.522 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2017.json.bz2 in 5.030 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2017.json.bz2 in 5.534 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2017.json.bz2 in 6.113 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2017.json.bz2 in 6.624 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2017.json.bz2 in 7.127 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2017.json.bz2 in 7.634 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2017.json.bz2 in 8.148 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2017.json.bz2 in 8.687 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2017.json.bz2 in 9.186 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2017.json.bz2 in 9.683 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2017.json.bz2 in 10.190 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2017.json.bz2 in 10.700 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2017.json.bz2 in 11.196 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2017.json.bz2 in 11.697 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2017.json.bz2 in 12.198 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2017.json.bz2 in 12.699 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2017.json.bz2 in 13.194 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2017.json.bz2 in 13.698 minutes\n",
      "Finished processing Data\\quotes-2017.json.bz2 in 14.015 minutes\n",
      "Starting processing Data\\quotes-2018.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2018.json.bz2 in 0.468 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2018.json.bz2 in 0.937 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2018.json.bz2 in 1.398 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2018.json.bz2 in 1.862 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2018.json.bz2 in 2.322 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2018.json.bz2 in 2.784 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2018.json.bz2 in 3.242 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2018.json.bz2 in 3.704 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2018.json.bz2 in 4.165 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2018.json.bz2 in 4.627 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2018.json.bz2 in 5.097 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2018.json.bz2 in 5.559 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2018.json.bz2 in 6.024 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2018.json.bz2 in 6.484 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2018.json.bz2 in 6.950 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2018.json.bz2 in 7.412 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2018.json.bz2 in 7.876 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2018.json.bz2 in 8.337 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2018.json.bz2 in 8.796 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2018.json.bz2 in 9.256 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2018.json.bz2 in 9.718 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2018.json.bz2 in 10.180 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2018.json.bz2 in 10.662 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2018.json.bz2 in 11.161 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2018.json.bz2 in 11.661 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2018.json.bz2 in 12.164 minutes\n",
      "Processed 27000000 lines from Data\\quotes-2018.json.bz2 in 12.704 minutes\n",
      "Finished processing Data\\quotes-2018.json.bz2 in 12.819 minutes\n",
      "Starting processing Data\\quotes-2019.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2019.json.bz2 in 0.470 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2019.json.bz2 in 0.945 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2019.json.bz2 in 1.439 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2019.json.bz2 in 1.882 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2019.json.bz2 in 2.369 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2019.json.bz2 in 2.846 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2019.json.bz2 in 3.311 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2019.json.bz2 in 3.762 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2019.json.bz2 in 4.348 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2019.json.bz2 in 4.888 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2019.json.bz2 in 5.344 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2019.json.bz2 in 5.796 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2019.json.bz2 in 6.298 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2019.json.bz2 in 6.833 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2019.json.bz2 in 7.330 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2019.json.bz2 in 7.818 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2019.json.bz2 in 8.307 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2019.json.bz2 in 8.781 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2019.json.bz2 in 9.274 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2019.json.bz2 in 9.735 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2019.json.bz2 in 10.201 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing Data\\quotes-2019.json.bz2 in 10.561 minutes\n",
      "Starting processing Data\\quotes-2020.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2020.json.bz2 in 0.484 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2020.json.bz2 in 0.973 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2020.json.bz2 in 1.451 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2020.json.bz2 in 1.918 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2020.json.bz2 in 2.377 minutes\n",
      "Finished processing Data\\quotes-2020.json.bz2 in 2.494 minutes\n",
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 0.456 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 0.927 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 1.367 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 1.817 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 2.282 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 2.732 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 3.214 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2015.json.bz2 in 3.692 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2015.json.bz2 in 4.163 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2015.json.bz2 in 4.614 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2015.json.bz2 in 5.072 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2015.json.bz2 in 5.550 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2015.json.bz2 in 6.028 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2015.json.bz2 in 6.493 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2015.json.bz2 in 6.949 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2015.json.bz2 in 7.399 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2015.json.bz2 in 7.869 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2015.json.bz2 in 8.342 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2015.json.bz2 in 8.809 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2015.json.bz2 in 9.262 minutes\n",
      "Finished processing Data\\quotes-2015.json.bz2 in 9.660 minutes\n",
      "Starting processing Data\\quotes-2016.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2016.json.bz2 in 0.478 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2016.json.bz2 in 0.954 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2016.json.bz2 in 1.425 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2016.json.bz2 in 1.901 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2016.json.bz2 in 2.381 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2016.json.bz2 in 2.844 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2016.json.bz2 in 3.337 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2016.json.bz2 in 3.820 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2016.json.bz2 in 4.314 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2016.json.bz2 in 4.808 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2016.json.bz2 in 5.280 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2016.json.bz2 in 5.752 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2016.json.bz2 in 6.208 minutes\n",
      "Finished processing Data\\quotes-2016.json.bz2 in 6.609 minutes\n",
      "Starting processing Data\\quotes-2017.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2017.json.bz2 in 0.568 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2017.json.bz2 in 1.116 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2017.json.bz2 in 1.655 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2017.json.bz2 in 2.175 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2017.json.bz2 in 2.697 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2017.json.bz2 in 3.226 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2017.json.bz2 in 3.745 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2017.json.bz2 in 4.262 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2017.json.bz2 in 4.808 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2017.json.bz2 in 5.337 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2017.json.bz2 in 5.848 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2017.json.bz2 in 6.412 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2017.json.bz2 in 6.964 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2017.json.bz2 in 7.634 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2017.json.bz2 in 8.198 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2017.json.bz2 in 8.793 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2017.json.bz2 in 9.306 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2017.json.bz2 in 9.819 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2017.json.bz2 in 10.336 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2017.json.bz2 in 10.889 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2017.json.bz2 in 11.426 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2017.json.bz2 in 11.976 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2017.json.bz2 in 12.494 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2017.json.bz2 in 13.030 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2017.json.bz2 in 13.546 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2017.json.bz2 in 14.057 minutes\n",
      "Finished processing Data\\quotes-2017.json.bz2 in 14.379 minutes\n",
      "Starting processing Data\\quotes-2018.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2018.json.bz2 in 0.481 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2018.json.bz2 in 0.984 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2018.json.bz2 in 1.474 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2018.json.bz2 in 1.970 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2018.json.bz2 in 2.443 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2018.json.bz2 in 2.930 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2018.json.bz2 in 3.406 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2018.json.bz2 in 3.886 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2018.json.bz2 in 4.383 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2018.json.bz2 in 4.875 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2018.json.bz2 in 5.393 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2018.json.bz2 in 5.898 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2018.json.bz2 in 6.390 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2018.json.bz2 in 6.895 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2018.json.bz2 in 7.385 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2018.json.bz2 in 7.881 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2018.json.bz2 in 8.401 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2018.json.bz2 in 8.928 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2018.json.bz2 in 9.473 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2018.json.bz2 in 10.018 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2018.json.bz2 in 10.574 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2018.json.bz2 in 11.098 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2018.json.bz2 in 11.611 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2018.json.bz2 in 12.112 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2018.json.bz2 in 12.614 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2018.json.bz2 in 13.120 minutes\n",
      "Processed 27000000 lines from Data\\quotes-2018.json.bz2 in 13.639 minutes\n",
      "Finished processing Data\\quotes-2018.json.bz2 in 13.750 minutes\n",
      "Starting processing Data\\quotes-2019.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2019.json.bz2 in 0.474 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2019.json.bz2 in 0.949 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2019.json.bz2 in 1.431 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2019.json.bz2 in 1.904 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2019.json.bz2 in 2.380 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2019.json.bz2 in 2.852 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2019.json.bz2 in 3.335 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2019.json.bz2 in 3.802 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2019.json.bz2 in 4.284 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2019.json.bz2 in 4.816 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2019.json.bz2 in 5.310 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2019.json.bz2 in 5.799 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2019.json.bz2 in 6.307 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 14000000 lines from Data\\quotes-2019.json.bz2 in 6.786 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2019.json.bz2 in 7.278 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2019.json.bz2 in 7.791 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2019.json.bz2 in 8.303 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2019.json.bz2 in 8.781 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2019.json.bz2 in 9.277 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2019.json.bz2 in 9.788 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2019.json.bz2 in 10.280 minutes\n",
      "Finished processing Data\\quotes-2019.json.bz2 in 10.648 minutes\n",
      "Starting processing Data\\quotes-2020.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2020.json.bz2 in 0.483 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2020.json.bz2 in 0.944 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2020.json.bz2 in 1.448 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2020.json.bz2 in 1.917 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2020.json.bz2 in 2.377 minutes\n",
      "Finished processing Data\\quotes-2020.json.bz2 in 2.491 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9220/745236423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_dataset_BERT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\ADA\\Project\\utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;31m# If necessary, compute the function output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mexecution_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9220/152385753.py\u001b[0m in \u001b[0;36membed_dataset_BERT\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_topic_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_quotes_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'quotation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    280\u001b[0m             self.embedding_model = select_backend(self.embedding_model,\n\u001b[0;32m    281\u001b[0m                                                   language=self.language)\n\u001b[1;32m--> 282\u001b[1;33m             embeddings = self._extract_embeddings(documents.Document,\n\u001b[0m\u001b[0;32m    283\u001b[0m                                                   \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"document\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m                                                   verbose=self.verbose)\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[1;34m(self, documents, method, verbose)\u001b[0m\n\u001b[0;32m   1333\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"document\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             raise ValueError(\"Wrong method for extracting document/word embeddings. \"\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\bertopic\\backend\\_base.py\u001b[0m in \u001b[0;36membed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \"\"\"\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 )\n\u001b[0;32m    582\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     ):\n\u001b[1;32m--> 400\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     ):\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model, (topics, probabilities) = embed_dataset_BERT(data_dir = DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0047411",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save(BERT_MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a330bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.get_topic_info().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6aab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(20):\n",
    "    print(f\"Topic {topic}:\")\n",
    "    print('\\n'.join(str(elem) for elem in bert_model.get_topic(topic)), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.visualize_hierarchy(top_n_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.visualize_barchart(top_n_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.visualize_heatmap(n_clusters=20, width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This declining going to right is sign that first words in each topic are very representative of whole topic\n",
    "bert_model.visualize_term_rank()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
