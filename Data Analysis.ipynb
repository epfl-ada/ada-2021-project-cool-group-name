{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a709e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "import bz2\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e901c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(data_dir, output_file_path, can_reuse_output = True):\n",
    "    if os.path.isfile(output_file_path) and can_reuse_output:\n",
    "        return\n",
    "    \n",
    "    filenames = [filename for filename in os.listdir(data_dir) if filename.endswith('.json.bz2')]\n",
    "    input_files_paths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "\n",
    "    domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "    get_domain_from_url = lambda url: domain_matcher.match(url).group('domain')\n",
    "    \n",
    "    with bz2.open(output_file_path, 'wb') as output_file:\n",
    "        for input_file_path in input_files_paths:\n",
    "            start = time.time()\n",
    "            \n",
    "            with bz2.open(input_file_path, 'rb') as input_file:\n",
    "                for i, line in enumerate(input_file):\n",
    "                    line = json.loads(line)\n",
    "                    \n",
    "                    data_line = {'quote_word_count': Counter(get_words(line['quotation'])),\n",
    "                                 'speaker'         : line['speaker'],\n",
    "                                 'qids'            : line['qids'],\n",
    "                                 'first_date'      : line['date'],\n",
    "                                 'num_occurrences' : line['numOccurrences'],\n",
    "                                 'domains'         : [get_domain_from_url(url) for url in line['urls']]}\n",
    "\n",
    "                    output_file.write((json.dumps(data_line) + '\\n').encode('utf-8'))\n",
    "                    \n",
    "                    if not i % 1000000:\n",
    "                        print(f\"Read {i} lines from {input_file_path} in {(time.time() - start) / 60:.3f} minutes\")\n",
    "                        \n",
    "            print(f\"Finished reading {input_file_path} in {(time.time() - start) / 60:.3f} minutes\")\n",
    "\n",
    "def load_col_from_json(input_file_path, columns):\n",
    "    \"\"\"\n",
    "    WARNING: DONT USE THIS UNLESS YOU WANT A BROKEN COMPUTER\n",
    "    \"\"\"\n",
    "    columns_dict = {key: [] for key in columns}\n",
    "    \n",
    "    with bz2.open(input_file_path, 'rb') as input_file:\n",
    "        start = time.time()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            line = json.loads(line)\n",
    "            \n",
    "            for column in columns:\n",
    "                columns_dict[column].append(line[column])\n",
    "                \n",
    "            if not i % 1000000:\n",
    "                print(f\"Read {i} lines from {input_file_path} in {(time.time() - start) / 60:.3f} minutes\")\n",
    "                \n",
    "    return pd.DataFrame(columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffc7c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 lines from Data\\quotes-2015.json.bz2 in 0.0004942695299784343 minutes\n",
      "Read 1000000 lines from Data\\quotes-2015.json.bz2 in 3.264639441172282 minutes\n",
      "Read 2000000 lines from Data\\quotes-2015.json.bz2 in 6.604445203145345 minutes\n",
      "Read 3000000 lines from Data\\quotes-2015.json.bz2 in 9.920422367254893 minutes\n",
      "Read 4000000 lines from Data\\quotes-2015.json.bz2 in 13.35372861623764 minutes\n",
      "Read 5000000 lines from Data\\quotes-2015.json.bz2 in 16.664416309197744 minutes\n",
      "Read 6000000 lines from Data\\quotes-2015.json.bz2 in 20.110660854975382 minutes\n",
      "Read 7000000 lines from Data\\quotes-2015.json.bz2 in 23.42496153116226 minutes\n",
      "Read 8000000 lines from Data\\quotes-2015.json.bz2 in 26.676215108235677 minutes\n",
      "Read 9000000 lines from Data\\quotes-2015.json.bz2 in 29.90466026067734 minutes\n",
      "Read 10000000 lines from Data\\quotes-2015.json.bz2 in 33.152570303281145 minutes\n",
      "Read 11000000 lines from Data\\quotes-2015.json.bz2 in 36.42050805886586 minutes\n",
      "Read 12000000 lines from Data\\quotes-2015.json.bz2 in 39.75851060549418 minutes\n",
      "Read 13000000 lines from Data\\quotes-2015.json.bz2 in 43.01414058208466 minutes\n",
      "Read 14000000 lines from Data\\quotes-2015.json.bz2 in 46.306911424795786 minutes\n",
      "Read 15000000 lines from Data\\quotes-2015.json.bz2 in 49.44390109380086 minutes\n",
      "Read 16000000 lines from Data\\quotes-2015.json.bz2 in 52.58403051694234 minutes\n",
      "Read 17000000 lines from Data\\quotes-2015.json.bz2 in 55.77077876726786 minutes\n",
      "Read 18000000 lines from Data\\quotes-2015.json.bz2 in 59.19052412509918 minutes\n",
      "Read 19000000 lines from Data\\quotes-2015.json.bz2 in 62.35735361973445 minutes\n",
      "Read 20000000 lines from Data\\quotes-2015.json.bz2 in 65.4836965560913 minutes\n",
      "Finished reading Data\\quotes-2015.json.bz2 in 68.25955404440562 minutes\n",
      "Read 0 lines from Data\\quotes-2016.json.bz2 in 0.0005000551541646321 minutes\n",
      "Read 1000000 lines from Data\\quotes-2016.json.bz2 in 3.743554695447286 minutes\n",
      "Read 2000000 lines from Data\\quotes-2016.json.bz2 in 7.553968171278636 minutes\n",
      "Read 3000000 lines from Data\\quotes-2016.json.bz2 in 10.9077702999115 minutes\n",
      "Read 4000000 lines from Data\\quotes-2016.json.bz2 in 14.112279597918192 minutes\n",
      "Read 5000000 lines from Data\\quotes-2016.json.bz2 in 17.315192719300587 minutes\n",
      "Read 6000000 lines from Data\\quotes-2016.json.bz2 in 20.503898469607034 minutes\n",
      "Read 7000000 lines from Data\\quotes-2016.json.bz2 in 23.712797649701436 minutes\n",
      "Read 8000000 lines from Data\\quotes-2016.json.bz2 in 26.911540154616038 minutes\n",
      "Read 9000000 lines from Data\\quotes-2016.json.bz2 in 30.133339103062948 minutes\n",
      "Read 10000000 lines from Data\\quotes-2016.json.bz2 in 33.345490380128226 minutes\n",
      "Read 11000000 lines from Data\\quotes-2016.json.bz2 in 36.551419325669606 minutes\n",
      "Read 12000000 lines from Data\\quotes-2016.json.bz2 in 39.74401386578878 minutes\n",
      "Read 13000000 lines from Data\\quotes-2016.json.bz2 in 42.95044041077296 minutes\n",
      "Finished reading Data\\quotes-2016.json.bz2 in 45.71739267110824 minutes\n",
      "Read 0 lines from Data\\quotes-2017.json.bz2 in 0.0006351312001546224 minutes\n",
      "Read 1000000 lines from Data\\quotes-2017.json.bz2 in 3.93058158159256 minutes\n",
      "Read 2000000 lines from Data\\quotes-2017.json.bz2 in 7.764987274010976 minutes\n",
      "Read 3000000 lines from Data\\quotes-2017.json.bz2 in 11.518795589605967 minutes\n",
      "Read 4000000 lines from Data\\quotes-2017.json.bz2 in 15.300498195489247 minutes\n",
      "Read 5000000 lines from Data\\quotes-2017.json.bz2 in 18.99312402009964 minutes\n",
      "Read 6000000 lines from Data\\quotes-2017.json.bz2 in 22.98496491909027 minutes\n",
      "Read 7000000 lines from Data\\quotes-2017.json.bz2 in 26.893337706724804 minutes\n",
      "Read 8000000 lines from Data\\quotes-2017.json.bz2 in 31.040455599625904 minutes\n",
      "Read 9000000 lines from Data\\quotes-2017.json.bz2 in 35.30209953387578 minutes\n",
      "Read 10000000 lines from Data\\quotes-2017.json.bz2 in 39.836917316913606 minutes\n",
      "Read 11000000 lines from Data\\quotes-2017.json.bz2 in 42.71779903968175 minutes\n",
      "Read 12000000 lines from Data\\quotes-2017.json.bz2 in 44.691037829717 minutes\n",
      "Read 13000000 lines from Data\\quotes-2017.json.bz2 in 46.64373293320338 minutes\n",
      "Read 14000000 lines from Data\\quotes-2017.json.bz2 in 48.61444837649663 minutes\n",
      "Read 15000000 lines from Data\\quotes-2017.json.bz2 in 50.61580711603165 minutes\n",
      "Read 16000000 lines from Data\\quotes-2017.json.bz2 in 52.73208758831024 minutes\n",
      "Read 17000000 lines from Data\\quotes-2017.json.bz2 in 54.72402644554774 minutes\n",
      "Read 18000000 lines from Data\\quotes-2017.json.bz2 in 56.6769091407458 minutes\n",
      "Read 19000000 lines from Data\\quotes-2017.json.bz2 in 58.676995599269866 minutes\n",
      "Read 20000000 lines from Data\\quotes-2017.json.bz2 in 60.7044223745664 minutes\n",
      "Read 21000000 lines from Data\\quotes-2017.json.bz2 in 62.67992202838262 minutes\n",
      "Read 22000000 lines from Data\\quotes-2017.json.bz2 in 64.65535191694896 minutes\n",
      "Read 23000000 lines from Data\\quotes-2017.json.bz2 in 66.66932125091553 minutes\n",
      "Read 24000000 lines from Data\\quotes-2017.json.bz2 in 68.65316592852274 minutes\n",
      "Read 25000000 lines from Data\\quotes-2017.json.bz2 in 70.58823981285096 minutes\n",
      "Read 26000000 lines from Data\\quotes-2017.json.bz2 in 72.51851203044255 minutes\n",
      "Finished reading Data\\quotes-2017.json.bz2 in 73.74695515235265 minutes\n",
      "Read 0 lines from Data\\quotes-2018.json.bz2 in 0.00023336410522460936 minutes\n",
      "Read 1000000 lines from Data\\quotes-2018.json.bz2 in 1.878485631942749 minutes\n",
      "Read 2000000 lines from Data\\quotes-2018.json.bz2 in 3.774592657883962 minutes\n",
      "Read 3000000 lines from Data\\quotes-2018.json.bz2 in 5.679440383116404 minutes\n",
      "Read 4000000 lines from Data\\quotes-2018.json.bz2 in 7.534065556526184 minutes\n",
      "Read 5000000 lines from Data\\quotes-2018.json.bz2 in 9.328207178910572 minutes\n",
      "Read 6000000 lines from Data\\quotes-2018.json.bz2 in 11.13934673468272 minutes\n",
      "Read 7000000 lines from Data\\quotes-2018.json.bz2 in 12.98530952533086 minutes\n",
      "Read 8000000 lines from Data\\quotes-2018.json.bz2 in 14.875340966383616 minutes\n",
      "Read 9000000 lines from Data\\quotes-2018.json.bz2 in 16.73303073644638 minutes\n",
      "Read 10000000 lines from Data\\quotes-2018.json.bz2 in 18.6251029809316 minutes\n",
      "Read 11000000 lines from Data\\quotes-2018.json.bz2 in 20.488227693239846 minutes\n",
      "Read 12000000 lines from Data\\quotes-2018.json.bz2 in 22.142874670028686 minutes\n",
      "Read 13000000 lines from Data\\quotes-2018.json.bz2 in 23.822521845499676 minutes\n",
      "Read 14000000 lines from Data\\quotes-2018.json.bz2 in 25.413522442181904 minutes\n",
      "Read 15000000 lines from Data\\quotes-2018.json.bz2 in 26.96177872419357 minutes\n",
      "Read 16000000 lines from Data\\quotes-2018.json.bz2 in 28.534654382864634 minutes\n",
      "Read 17000000 lines from Data\\quotes-2018.json.bz2 in 30.11013484398524 minutes\n",
      "Read 18000000 lines from Data\\quotes-2018.json.bz2 in 31.656252535184226 minutes\n",
      "Read 19000000 lines from Data\\quotes-2018.json.bz2 in 33.279403591156004 minutes\n",
      "Read 20000000 lines from Data\\quotes-2018.json.bz2 in 34.88591754039128 minutes\n",
      "Read 21000000 lines from Data\\quotes-2018.json.bz2 in 36.51035221815109 minutes\n",
      "Read 22000000 lines from Data\\quotes-2018.json.bz2 in 38.13404376506806 minutes\n",
      "Read 23000000 lines from Data\\quotes-2018.json.bz2 in 39.75346064170201 minutes\n",
      "Read 24000000 lines from Data\\quotes-2018.json.bz2 in 41.40648837486903 minutes\n",
      "Read 25000000 lines from Data\\quotes-2018.json.bz2 in 43.066923507054646 minutes\n",
      "Read 26000000 lines from Data\\quotes-2018.json.bz2 in 44.777980236212414 minutes\n",
      "Read 27000000 lines from Data\\quotes-2018.json.bz2 in 46.3584704319636 minutes\n",
      "Finished reading Data\\quotes-2018.json.bz2 in 46.7088831504186 minutes\n",
      "Read 0 lines from Data\\quotes-2019.json.bz2 in 0.0005836248397827149 minutes\n",
      "Read 1000000 lines from Data\\quotes-2019.json.bz2 in 1.5469006021817526 minutes\n",
      "Read 2000000 lines from Data\\quotes-2019.json.bz2 in 3.0593766808509826 minutes\n",
      "Read 3000000 lines from Data\\quotes-2019.json.bz2 in 4.5517314354578655 minutes\n",
      "Read 4000000 lines from Data\\quotes-2019.json.bz2 in 6.068370195229848 minutes\n",
      "Read 5000000 lines from Data\\quotes-2019.json.bz2 in 7.589748195807139 minutes\n",
      "Read 6000000 lines from Data\\quotes-2019.json.bz2 in 9.117777705192566 minutes\n",
      "Read 7000000 lines from Data\\quotes-2019.json.bz2 in 10.611932873725891 minutes\n",
      "Read 8000000 lines from Data\\quotes-2019.json.bz2 in 12.085807466506958 minutes\n",
      "Read 9000000 lines from Data\\quotes-2019.json.bz2 in 13.585447216033936 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10000000 lines from Data\\quotes-2019.json.bz2 in 15.0700168689092 minutes\n",
      "Read 11000000 lines from Data\\quotes-2019.json.bz2 in 16.55344971815745 minutes\n",
      "Read 12000000 lines from Data\\quotes-2019.json.bz2 in 18.029754054546355 minutes\n",
      "Read 13000000 lines from Data\\quotes-2019.json.bz2 in 19.521382276217143 minutes\n",
      "Read 14000000 lines from Data\\quotes-2019.json.bz2 in 21.00143426656723 minutes\n",
      "Read 15000000 lines from Data\\quotes-2019.json.bz2 in 22.474851389726002 minutes\n",
      "Read 16000000 lines from Data\\quotes-2019.json.bz2 in 24.064259060223897 minutes\n",
      "Read 17000000 lines from Data\\quotes-2019.json.bz2 in 25.58055257399877 minutes\n",
      "Read 18000000 lines from Data\\quotes-2019.json.bz2 in 27.096012564500175 minutes\n",
      "Read 19000000 lines from Data\\quotes-2019.json.bz2 in 28.58882019519806 minutes\n",
      "Read 20000000 lines from Data\\quotes-2019.json.bz2 in 30.06763856013616 minutes\n",
      "Read 21000000 lines from Data\\quotes-2019.json.bz2 in 31.552624980608623 minutes\n",
      "Finished reading Data\\quotes-2019.json.bz2 in 32.70917032559713 minutes\n",
      "Read 0 lines from Data\\quotes-2020.json.bz2 in 0.00028705199559529623 minutes\n",
      "Read 1000000 lines from Data\\quotes-2020.json.bz2 in 1.4452143987019856 minutes\n",
      "Read 2000000 lines from Data\\quotes-2020.json.bz2 in 2.9220323006312054 minutes\n",
      "Read 3000000 lines from Data\\quotes-2020.json.bz2 in 4.41097819407781 minutes\n",
      "Read 4000000 lines from Data\\quotes-2020.json.bz2 in 5.940374672412872 minutes\n",
      "Read 5000000 lines from Data\\quotes-2020.json.bz2 in 7.492026182015737 minutes\n",
      "Finished reading Data\\quotes-2020.json.bz2 in 7.869578385353089 minutes\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "CACHE_FILE_PATH = os.path.join(CACHE_DIR, 'processed_data.json.bz2')\n",
    "\n",
    "process_files(DATA_DIR, CACHE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e6189",
   "metadata": {},
   "source": [
    "# POUR KAOU\n",
    "\n",
    "Bonjour Kaou, tu as choisi word counting. Si tu n'as pas lu ça assez tôt, j'ai déjà eu le temps de relancer la cellule du haut et enlever la punctuation plus mettre tous les mots en minuscule pour toi.\n",
    "\n",
    "Lis cette page pour des idées sur comment faire évoluer ton travail au délà du word counting: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Je te mets aussi à disposition une simple fonction qui efface les \"English stop words\" d'une liste de mots. Par contre, fais gaffe à comment tu l'utilises. Les stop words ne sont pas nécéssairement toutes toujours inutiles. Lis sklearn pour les détails et affiche la liste avant d'utiliser la fonction pour voir si elle t'arrange. Tu peux covertir la fonction pour qu'elle droppe le compte des stop words dans un compteur en faisant del counter_object[word]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94165174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def get_words(string):\n",
    "    # Make all lower-case.\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove punctuation.\n",
    "    string = re.sub(r'[^a-z0-9 ]', '', string)\n",
    "    \n",
    "    # Split into words.\n",
    "    words = string.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    return [word for word in word_list if word not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290c689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc82165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c788fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3610133d",
   "metadata": {},
   "source": [
    "# POUR CÉLINA\n",
    "\n",
    "Coucou CC, il nous ont donné un file .parquet (juste un autre format de stockage binaire, un peux comme pickle) que normalement j'ai mis dans le dossier Data. Il devrait contenir plusieures informations sur chaque speaker. Tu peux le load comme ça, mais il me semble qu'il faut installer pyarrow (  conda install -c conda-forge pyarrow  ).\n",
    "\n",
    "Malheureusement il parait qu'ils aient utilisé le QID de Wikidata pour encoder la profession, religion et tout le reste. Probablement une des première choses à faire est de trouver un moyen de mapper les QIDS à des string, que je sais faire en faisant des queries à wikidata mais je me demande s'il n'y a pas un moyen plus simple. Au pire on le fait (que une fois de toute façon).\n",
    "\n",
    "Il faudrait aussi s'assurer que dans le parquet qu'ils nous donnent il n'y ait pas que peu d'occupations au bol, mais un grand nombre comme celui qu'on voit sur wikidata, car autrement on a le même problème que quand je faisais les queries (que j'ai découvert comment resudre) qui est que pour des gens avec beaucoup de professions on en avait seulement 3 au bol.\n",
    "\n",
    "Oubliepas que je t'ai dit qu'il y a un problème avec l'antivirus et la lecture de .json. Je ne sais pas pourquoi, mais à chaque fois que tu veux commencer à lire un json tu dois mettre en standby l'anti-virus pendant une minute. C'est assez chiant, mais je n'ai pas trouvé d'autre solution (sinon, le programme ne plante pas, il s'arrete juste et ne dit rien et attend jusqu'à que tu desactives).\n",
    "\n",
    "Pour le moment c'est tout je crois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95724d7",
   "metadata": {},
   "source": [
    "- Cas quand plusieurs qids par quote, lequel on prend ? (je sais pas s'il y a moyen de savoir laquelle est la plus populaire, peut-être celle qui a le plus de liens externes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d397305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.cache_to_file_pickle(\"function-groupby_speaker\", cache_dir = CACHE_DIR)\n",
    "def groupby_speaker(input_file_path):\n",
    "    speakers_dict = {}\n",
    "    \n",
    "    with bz2.open(input_file_path, 'rb') as input_file:\n",
    "        start = time.time()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            line = json.loads(line)\n",
    "            \n",
    "            if not line['qids']:\n",
    "                continue\n",
    "            \n",
    "            qids = tuple(line['qids']) if len(line['qids']) > 1 else line['qids'][0]\n",
    "            \n",
    "            # if multiple qids given for one quote, take the first one for now\n",
    "            if qids in speakers_dict:\n",
    "                speakers_dict[qids]['quote_count'] += 1\n",
    "                speakers_dict[qids]['speaker']     |= set([line['speaker']])\n",
    "                speakers_dict[qids]['num_occurrences'].append(line['numOccurrences'])\n",
    "                                \n",
    "            else:\n",
    "                speakers_dict[qids] = {'quote_count': 1, \\\n",
    "                                       'speaker': set([line['speaker']]),\n",
    "                                       'num_occurrences': [line['numOccurrences']]}\n",
    "                \n",
    "                \n",
    "            if not i % 1000000:\n",
    "                print(\"Read\", i, \"lines from\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "\n",
    "    return speakers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8648a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 lines from Cache\\processed_data.json.bz2 in 0.00019950469334920247 minutes\n",
      "Read 1000000 lines from Cache\\processed_data.json.bz2 in 0.37840535640716555 minutes\n"
     ]
    }
   ],
   "source": [
    "data_quotes = groupby_speaker(input_file_path = CACHE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b65431",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quotes_df = pd.DataFrame(data_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00ddd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_count</th>\n",
       "      <th>speaker</th>\n",
       "      <th>num_occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q270316</th>\n",
       "      <td>30</td>\n",
       "      <td>{Jeanne Shaheen}</td>\n",
       "      <td>[2, 2, 1, 2, 1, 1, 2, 2, 3, 2, 1, 1, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1253</th>\n",
       "      <td>339</td>\n",
       "      <td>{Ban Ki Moon, Ban Ki-Moon, Ban Ki-moon, Ban ki...</td>\n",
       "      <td>[2, 1, 99, 1, 2, 1, 2, 91, 7, 1, 3, 2, 12, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q468374</th>\n",
       "      <td>5</td>\n",
       "      <td>{Sri Sri Ravi Shankar}</td>\n",
       "      <td>[1, 5, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q19874690</th>\n",
       "      <td>5</td>\n",
       "      <td>{Jamal Rifi}</td>\n",
       "      <td>[1, 1, 1, 8, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q18601741</th>\n",
       "      <td>1</td>\n",
       "      <td>{Richard Burmeister}</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5271548</th>\n",
       "      <td>9</td>\n",
       "      <td>{Diane Ravitch}</td>\n",
       "      <td>[1, 1, 3, 1, 1, 2, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Q15735939, Q25189328, Q5107375, Q5110828, Q5112832, Q948687)</th>\n",
       "      <td>55</td>\n",
       "      <td>{Chris Matthews}</td>\n",
       "      <td>[1, 1, 3, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q51797519</th>\n",
       "      <td>5</td>\n",
       "      <td>{Steven Stack}</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2287947</th>\n",
       "      <td>292</td>\n",
       "      <td>{JORDAN Spieth, Jordan Spieth}</td>\n",
       "      <td>[1, 6, 13, 9, 1, 1, 1, 1, 1, 47, 14, 1, 1, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Q2376327, Q313381, Q7815037)</th>\n",
       "      <td>368</td>\n",
       "      <td>{TOM BRADY, Tom Brady}</td>\n",
       "      <td>[1, 1, 1, 1, 7, 5, 1, 2, 1, 10, 1, 2, 2, 1, 4,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   quote_count  \\\n",
       "Q270316                                                     30   \n",
       "Q1253                                                      339   \n",
       "Q468374                                                      5   \n",
       "Q19874690                                                    5   \n",
       "Q18601741                                                    1   \n",
       "Q5271548                                                     9   \n",
       "(Q15735939, Q25189328, Q5107375, Q5110828, Q511...          55   \n",
       "Q51797519                                                    5   \n",
       "Q2287947                                                   292   \n",
       "(Q2376327, Q313381, Q7815037)                              368   \n",
       "\n",
       "                                                                                              speaker  \\\n",
       "Q270316                                                                              {Jeanne Shaheen}   \n",
       "Q1253                                               {Ban Ki Moon, Ban Ki-Moon, Ban Ki-moon, Ban ki...   \n",
       "Q468374                                                                        {Sri Sri Ravi Shankar}   \n",
       "Q19874690                                                                                {Jamal Rifi}   \n",
       "Q18601741                                                                        {Richard Burmeister}   \n",
       "Q5271548                                                                              {Diane Ravitch}   \n",
       "(Q15735939, Q25189328, Q5107375, Q5110828, Q511...                                   {Chris Matthews}   \n",
       "Q51797519                                                                              {Steven Stack}   \n",
       "Q2287947                                                               {JORDAN Spieth, Jordan Spieth}   \n",
       "(Q2376327, Q313381, Q7815037)                                                  {TOM BRADY, Tom Brady}   \n",
       "\n",
       "                                                                                      num_occurrences  \n",
       "Q270316                                             [2, 2, 1, 2, 1, 1, 2, 2, 3, 2, 1, 1, 1, 1, 2, ...  \n",
       "Q1253                                               [2, 1, 99, 1, 2, 1, 2, 91, 7, 1, 3, 2, 12, 1, ...  \n",
       "Q468374                                                                               [1, 5, 1, 1, 1]  \n",
       "Q19874690                                                                             [1, 1, 1, 8, 2]  \n",
       "Q18601741                                                                                         [1]  \n",
       "Q5271548                                                                  [1, 1, 3, 1, 1, 2, 1, 1, 1]  \n",
       "(Q15735939, Q25189328, Q5107375, Q5110828, Q511...  [1, 1, 3, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, ...  \n",
       "Q51797519                                                                             [1, 1, 1, 1, 1]  \n",
       "Q2287947                                            [1, 6, 13, 9, 1, 1, 1, 1, 1, 47, 14, 1, 1, 10,...  \n",
       "(Q2376327, Q313381, Q7815037)                       [1, 1, 1, 1, 7, 5, 1, 2, 1, 10, 1, 2, 2, 1, 4,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quotes_df.T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9019af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_data = pd.read_parquet('Data/speaker_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c23656c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_data.id.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6627eb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliases</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7078648</th>\n",
       "      <td>[Christopher Douglas Matthews]</td>\n",
       "      <td>[+1989-10-06T00:00:00Z]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1317867389</td>\n",
       "      <td>[Q49085]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q19204627, Q19841381]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q15735939</td>\n",
       "      <td>Chris Matthews</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                aliases            date_of_birth nationality  \\\n",
       "7078648  [Christopher Douglas Matthews]  [+1989-10-06T00:00:00Z]       [Q30]   \n",
       "\n",
       "             gender   lastrevid ethnic_group US_congress_bio_ID  \\\n",
       "7078648  [Q6581097]  1317867389     [Q49085]               None   \n",
       "\n",
       "                     occupation party academic_degree         id  \\\n",
       "7078648  [Q19204627, Q19841381]  None            None  Q15735939   \n",
       "\n",
       "                  label candidacy  type religion  \n",
       "7078648  Chris Matthews      None  item     None  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_data.loc[speaker_data[\"id\"] == 'Q15735939']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2818791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f86fda96",
   "metadata": {},
   "source": [
    "# POUR ANDREA\n",
    "Bonjour Andrea, ça va? Oui très bien, merci. J'essaye de poser une base pour commencer le Milestone 2 du projet. Et toi? Moi aussi, drôle ça. Bon, à toute. Bon travail, à toute.\n",
    "\n",
    "Il faut rerun la lecture du dataset avec la remotion de ponctuation.\n",
    "\n",
    "Je suppose que tu vas faire la partie de correler les dates à des événements, et si t'as envie d'essayer d'extraire la variance. Par contre pour le moment je ne sais pas trop comment ça colle avec le reste de l'analyse. Dans le sens que la data story va parler de quoi concernant les dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92973258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851cc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d6612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00c7d9d3",
   "metadata": {},
   "source": [
    "# POUR MATTIA\n",
    "Buongiorno Mattia, je suppose que tu vas faire la partie de regarder les newspapers et correler avec les speakers et, si on arrive, l'argument de la quote. Pour toi je crois que juste load le processed_data.json.bz2 et garder que les clés 'domains' et 'speaker' devrait le faire. Il faudra sûrement se coordonner avec Célina et Kaou pour voir justement comment correler les trucs. Pour le moment, je t'avoue que comme pour ma partie, je n'ai pas une idéé précide de comment ceci va coller dans une data story coherente. Faudra que Kaou et Célina avancent rapidement pour que tu puisse commencer à repliquer leur travail mais pour différents newspapers. Ou je ne sais pas. Faudra juste pas qu'on reste bloqués si on attend quelqu'un d'autre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
