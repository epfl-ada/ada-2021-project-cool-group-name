{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a709e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "import bz2\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e901c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(data_dir, output_file_path, can_reuse_output = True):\n",
    "    if os.path.isfile(output_file_path) and can_reuse_output:\n",
    "        return\n",
    "    \n",
    "    filenames = [filename for filename in os.listdir(data_dir) if filename.endswith('.json.bz2')]\n",
    "    input_files_paths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "\n",
    "    domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "    get_domain_from_url = lambda string: domain_matcher.match(string).group('domain')\n",
    "        \n",
    "    with bz2.open(output_file_path, 'wb') as output_file:\n",
    "        for input_file_path in input_files_paths:\n",
    "            start = time.time()\n",
    "            \n",
    "            with bz2.open(input_file_path, 'rb') as input_file:\n",
    "                for i, line in enumerate(input_file):\n",
    "                    line = json.loads(line)\n",
    "                    \n",
    "                    data_line = {'quote_word_count': Counter(line['quotation']),\n",
    "                                 'speaker'         : line['speaker'],\n",
    "                                 'qids'            : line['qids'],\n",
    "                                 'first_date'      : line['date'],\n",
    "                                 'domains'         : [get_domain_from_url(url) for url in line['urls']]}\n",
    "\n",
    "                    output_file.write((json.dumps(data_line) + '\\n').encode('utf-8'))\n",
    "                    \n",
    "                    if not i % 1000000:\n",
    "                        print(\"Read\", i, \"lines from\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "                        \n",
    "            print(\"Finished reading\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "\n",
    "process_files(DATA_DIR, os.path.join(CACHE_DIR, 'processed_data.json.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e6189",
   "metadata": {},
   "source": [
    "# POUR KAOU\n",
    "\n",
    "Bonjour Kaou, tu as choisi word counting. Malheureusement je me suis rendu trop tard dans l'execution de la cellule précédente que j'avais oublié d'enlever les punctuation. Oups. Tu vas devoir itérer sur tous les clés du Compteur et enlever manuellement, et merge aussi les mêmes mots. Ou juste relancer la cellule du haut que j'ai déjà corrigé, mais que je n'ai pas eu le courage de relancer parce que ça prend beaucoup trop de temps (mais pas de resources, donc ne te fais pas de soucis tu peux le run sur n'importe quoi).\n",
    "\n",
    "Lis cette page pour des idées sur comment faire évoluer ton travail au délà du word counting: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Je te mets aussi à disposition une simple fonction qui efface les \"English stop words\" d'une liste de mots. Par contre, fais gaffe à comment tu l'utilises. Les stop words ne sont pas nécéssairement toutes toujours inutiles. Lis sklearn pour les détails et affiche la liste avant d'utiliser la fonction pour voir si elle t'arrange. Tu peux covertir la fonction pour qu'elle droppe le compte des stop words dans un compteur en faisant del counter_object[word]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94165174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    return re.sub(r'[^a-z0-9 ]', '', string)\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    return [word for word in word_list if word not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290c689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc82165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c788fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3610133d",
   "metadata": {},
   "source": [
    "# POUR CÉLINA\n",
    "\n",
    "Coucou CC, il nous ont donné un file .parquet (juste un autre format de stockage binaire, un peux comme pickle) que normalement j'ai mis dans le dossier Data. Il devrait contenir plusieures informations sur chaque speaker. Tu peux le load comme ça, mais il me semble qu'il faut installer pyarrow (  conda install -c conda-forge pyarrow  ).\n",
    "\n",
    "Malheureusement il parait qu'ils aient utilisé le QID de Wikidata pour encoder la profession, religion et tout le reste. Probablement une des première choses à faire est de trouver un moyen de mapper les QIDS à des string, que je sais faire en faisant des queries à wikidata mais je me demande s'il n'y a pas un moyen plus simple. Au pire on le fait (que une fois de toute façon).\n",
    "\n",
    "Il faudrait aussi s'assurer que dans le parquet qu'ils nous donnent il n'y ait pas que peu d'occupations au bol, mais un grand nombre comme celui qu'on voit sur wikidata, car autrement on a le même problème que quand je faisais les queries (que j'ai découvert comment resudre) qui est que pour des gens avec beaucoup de professions on en avait seulement 3 au bol.\n",
    "\n",
    "Oubliepas que je t'ai dit qu'il y a un problème avec l'antivirus et la lecture de .json. Je ne sais pas pourquoi, mais à chaque fois que tu veux commencer à lire un json tu dois mettre en standby l'anti-virus pendant une minute. C'est assez chiant, mais je n'ai pas trouvé d'autre solution (sinon, le programme ne plante pas, il s'arrete juste et ne dit rien et attend jusqu'à que tu desactives).\n",
    "\n",
    "Pour le moment c'est tout je crois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Data/speaker_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23656c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627eb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2818791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f86fda96",
   "metadata": {},
   "source": [
    "# POUR ANDREA\n",
    "Bonjour Andrea, ça va? Oui très bien, merci. J'essaye de poser une base pour commencer le Milestone 2 du projet. Et toi? Moi aussi, drôle ça. Bon, à toute. Bon travail, à toute.\n",
    "\n",
    "Il faut rerun la lecture du dataset avec la remotion de ponctuation.\n",
    "\n",
    "Je suppose que tu vas faire la partie de correler les dates à des événements, et si t'as envie d'essayer d'extraire la variance. Par contre pour le moment je ne sais pas trop comment ça colle avec le reste de l'analyse. Dans le sens que la data story va parler de quoi concernant les dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92973258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00c7d9d3",
   "metadata": {},
   "source": [
    "# POUR MATTIA\n",
    "Buongiorno Mattia, je suppose que tu vas faire la partie de regarder les newspapers et correler avec les speakers et, si on arrive, l'argument de la quote. Pour toi je crois que juste load le processed_data.json.bz2 et garder que les clés 'domains' et 'speaker' devrait le faire. Il faudra sûrement se coordonner avec Célina et Kaou pour voir justement comment correler les trucs. Pour le moment, je t'avoue que comme pour ma partie, je n'ai pas une idéé précide de comment ceci va coller dans une data story coherente. Faudra que Kaou et Célina avancent rapidement pour que tu puisse commencer à repliquer leur travail mais pour différents newspapers. Ou je ne sais pas. Faudra juste pas qu'on reste bloqués si on attend quelqu'un d'autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04db7c8c-531b-4c69-a08c-c032233ea5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 lines from quotes-2019-nytimes.json.bz2 in 0.00034999847412109375 minutes\n",
      "Finished reading quotes-2019-nytimes.json.bz2 in 0.28105957905451456 minutes\n"
     ]
    }
   ],
   "source": [
    "#Create a counter for the different domains of the urls\n",
    "import bz2\n",
    "import json\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "input_file_path = 'quotes-2019-nytimes.json.bz2' \n",
    "\n",
    "\n",
    "domain=Counter()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with bz2.open(input_file_path, 'rb') as input_file:\n",
    "    for i, line in enumerate(input_file):\n",
    "        line = json.loads(line)\n",
    "        domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "        get_domain_from_url = lambda url: domain_matcher.match(url).group('domain')\n",
    "\n",
    "        domain.update([get_domain_from_url(url) for url in line['urls']])\n",
    "\n",
    "\n",
    "        if not i % 1000000:\n",
    "            print(\"Read\", i, \"lines from\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "\n",
    "print(\"Finished reading\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "efeb2d93-b72f-4f90-b386-2ea71ac8ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes.com: 213819\n",
      "msn.com: 45173\n",
      "uspolitics.einnews.com: 40377\n",
      "mobile.nytimes.com: 33893\n",
      "pulse.com.gh: 20466\n",
      "pulse.ng: 20404\n",
      "krmg.com: 12298\n",
      "wokv.com: 12260\n",
      "seattletimes.com: 11959\n",
      "breitbart.com: 11551\n"
     ]
    }
   ],
   "source": [
    "for k, v in domain.most_common(10):\n",
    "    print ('%s: %i' % (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3e916ccf-2493-4a59-9b17-0ed98714bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 lines from quotes-2019-nytimes.json.bz2 in 0.00040004650751749676 minutes\n",
      "Finished reading quotes-2019-nytimes.json.bz2 in 0.34906006256739297 minutes\n",
      "Percentage of date extraction:  27.257588417657765\n"
     ]
    }
   ],
   "source": [
    "#Extract dates and look at the percentrage of extraction for total urls\n",
    "input_file_path = 'quotes-2019-nytimes.json.bz2' \n",
    "output_file_path='test_dates.json.bz2'\n",
    "\n",
    "\n",
    "matches=0\n",
    "nomatch=0\n",
    "total_urls=0\n",
    "date=[]\n",
    "start = time.time()\n",
    "with bz2.open(output_file_path, 'wb') as output_file:\n",
    "    with bz2.open(input_file_path, 'rb') as input_file:\n",
    "        for i, line in enumerate(input_file):\n",
    "            line = json.loads(line)\n",
    "            total_urls+=len(line['urls'])\n",
    "            dates=[]\n",
    "            for url in line['urls']:\n",
    "                match=re.search('\\d{4}/\\d{2}/\\d{2}',url)\n",
    "                if match is not None :\n",
    "                    matches+=1\n",
    "                    dates.append(match.group())\n",
    "                    \n",
    "            data_line = {'Dates': dates}\n",
    "\n",
    "            output_file.write((json.dumps(data_line) + '\\n').encode('utf-8'))                    \n",
    "            if not i % 1000000:\n",
    "                print(\"Read\", i, \"lines from\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "\n",
    "print(\"Finished reading\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "\n",
    "print(\"Percentage of date extraction: \",matches/total_urls*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468dc103-fd16-4a8a-814e-40fa2a5637b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
