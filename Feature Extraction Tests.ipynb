{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "960f4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Importing useful packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import bz2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse\n",
    "from random import randrange\n",
    "from bertopic import BERTopic\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Importing our utilitary modules.\n",
    "import src.utils as utils\n",
    "import src.feature_extraction as feature_extraction\n",
    "import src.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffc7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants.\n",
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "SPEAKER_INFO_FILE_PATH = os.path.join(DATA_DIR, 'speaker_attributes.parquet')\n",
    "PREPROCESSED_DATASET_FILE_PATH = os.path.join(CACHE_DIR, \"preprocessed_dataset.json.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2abbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.cache_to_file_pickle(\"function-make_features_labels_arrays\", cache_dir = CACHE_DIR, ignore_kwargs = ['batch_size'])\n",
    "def make_features_labels_arrays(preprocessed_dataset_file_path, batch_size = 1000000):\n",
    "    \n",
    "    def process_batch(features, quotation_topic, num_occurrences,\n",
    "                      features_batch, quotations_batch, num_occurrences_batch):\n",
    "                \n",
    "        # If list is empty, don't do anything otherwise BERTopic raises un exception.\n",
    "        if features_batch:\n",
    "            topics, _ = bert_model.transform(quotations_batch)\n",
    "\n",
    "            # Convert sparse list of lists to COO sparse matrix. \n",
    "            # COO sparse matrices have the advantage of being quick to generate and to stack.\n",
    "            features.append( scipy.sparse.coo_matrix(features_batch) )\n",
    "\n",
    "            # These can simply be added at the end of the list as they are not sparse.\n",
    "            quotation_topic.extend( topics )\n",
    "            num_occurrences.extend( num_occurrences_batch )\n",
    "\n",
    "        return features, quotation_topic, num_occurrences\n",
    "    \n",
    "    features_cols_titles = None\n",
    "    \n",
    "    features = []\n",
    "    quotation_topic = []\n",
    "    num_occurrences = []\n",
    "    \n",
    "    features_batch = []\n",
    "    quotations_batch = []\n",
    "    num_occurrences_batch = []\n",
    "        \n",
    "    for line in utils.json_lines_generator(preprocessed_dataset_file_path):        \n",
    "        num_occurrences_batch.append(line.pop('num_occurrences'))\n",
    "        quotations_batch.append(line.pop('quotation'))\n",
    "        \n",
    "        if features_cols_titles is None:\n",
    "            features_cols_titles = sorted(line)\n",
    "\n",
    "        features_batch.append([line[key] for key in features_cols_titles])\n",
    "        \n",
    "        if len(features_batch) >= batch_size:\n",
    "            features, quotation_topic, num_occurrences = process_batch(features, quotation_topic, num_occurrences,\n",
    "                                                                       features_batch, quotations_batch, num_occurrences_batch)\n",
    "            features_batch = []\n",
    "            quotations_batch = []\n",
    "            num_occurrences_batch = []\n",
    "            \n",
    "    features, quotation_topic, num_occurrences = process_batch(features, quotation_topic, num_occurrences,\n",
    "                                                               features_batch, quotations_batch, num_occurrences_batch)\n",
    "\n",
    "    # Merge list of sparse matrices into a single sparse matrix.\n",
    "    # Here COO sparse format is useful because this stacking is done faster than with other formats.\n",
    "    features = scipy.sparse.vstack(features)\n",
    "    \n",
    "    # Convert list to a single numpy array.\n",
    "    quotation_topic = np.array(quotation_topic)\n",
    "    num_occurrences = np.array(num_occurrences)\n",
    "        \n",
    "    # One-hot encode quotation topics.\n",
    "    topic_number_names_map = bert_model.get_topic_info().set_index('Topic')\n",
    "    topic_number_names_map.at[-1, 'Name'] = 'unknown'\n",
    "\n",
    "    quotation_topic_names = topic_number_names_map.loc[quotation_topic, 'Name'].values\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse = True, dtype = 'int')\n",
    "    one_hot_quotation_topics = encoder.fit_transform(np.reshape(quotation_topic_names, (-1, 1)))\n",
    "    \n",
    "    # Add quotation topics as columns of feature matrix.\n",
    "    features = scipy.sparse.hstack([features, one_hot_quotation_topics])\n",
    "    encoder_categories, = encoder.categories_\n",
    "    features_cols_titles.extend([f'quotation_topic_{col_name.upper()}' for col_name in encoder_categories])\n",
    "     \n",
    "    # Convert sparse matrix from COO to CSR format for easy slicing later on.\n",
    "    features = features.tocsr()\n",
    "    \n",
    "    return features, num_occurrences, features_cols_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfccf16",
   "metadata": {},
   "source": [
    "## 3.B.a Big Function Doing all the stuff needed for one full run with a soecific viral_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2562d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aabe554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_test_split(VIRAL_THR):\n",
    "    # Load inside function to actually save RAM when deleting them.\n",
    "    features, num_occurrences, features_cols_titles = make_features_labels_arrays(preprocessed_dataset_file_path = PREPROCESSED_DATASET_FILE_PATH)\n",
    "\n",
    "    # Train-Test stratified split with test ratio of 0.3\n",
    "    viral_label = np.int32(num_occurrences > VIRAL_THR)\n",
    "\n",
    "    print(f\"Viral Thr: {VIRAL_THR}, {viral_label.mean():.3%} quotes are viral\")    \n",
    "\n",
    "    splits = train_test_split(features, num_occurrences, viral_label,\n",
    "                              test_size = 0.3,\n",
    "                              shuffle = True,\n",
    "                              stratify = viral_label)\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "def sweep_l1_regularization(features, labels, cv_n_splits = 10, downsampling_factor = 1,\n",
    "                            resample_balanced = False, n_jobs = 4,\n",
    "                            l1_log_min = -5, l1_log_max = 2, l1_log_steps = 50):\n",
    "\n",
    "    if downsampling_factor != 1:\n",
    "        # Use train_test_split to downsample dataset in a stratified manner.\n",
    "        _, features, _, labels = train_test_split(features, labels,\n",
    "                                                  test_size = 1. / downsampling_factor,\n",
    "                                                  shuffle = True,\n",
    "                                                  stratify = labels)\n",
    "        \n",
    "    if resample_balanced:\n",
    "        features, labels = SMOTETomek().fit_resample(features, labels)\n",
    "    \n",
    "    print(f\"Training on {len(labels)} samples with {features.shape[1]} features\")\n",
    "    print(f\"Positive samples: {np.sum(labels == 1)}, negative samples: {np.sum(labels == 0)}\")\n",
    "\n",
    "    res = GridSearchCV(estimator  = LinearSVC(dual = False, max_iter = 10000, class_weight = 'balanced'), \n",
    "                       param_grid = {'penalty': ('l1', ), 'C': np.logspace(l1_log_min, l1_log_max, l1_log_steps)},\n",
    "                       cv         = StratifiedKFold(cv_n_splits),\n",
    "                       scoring    = {'accuracy' : 'accuracy',\n",
    "                                     'precision': 'precision',\n",
    "                                     'recall'   : 'recall',\n",
    "                                     'f1'       : 'f1',\n",
    "                                     'sparsity' : lambda estimator, X, y: np.mean(np.abs(estimator.coef_) < 1e-8)}, \n",
    "                       return_train_score = True,\n",
    "                       refit = False,\n",
    "                       n_jobs = n_jobs,\n",
    "                       verbose = 2).fit(features, labels)\n",
    "\n",
    "    df = pd.DataFrame(res.cv_results_)\n",
    "    df = df[[col for col in df.columns if col.startswith('param_') or re.match(r'^(mean|std)_(train|test)_', col)]]\n",
    "\n",
    "    # Changing names of sparsity columns and dropping duplicates.\n",
    "    df = df.drop(columns = ['mean_test_sparsity', 'std_test_sparsity']).rename(columns = {'mean_train_sparsity': 'mean_sparsity',\n",
    "                                                                                          'std_train_sparsity' : 'std_sparsity'})\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sweep_l1_regularization_this_one_is_old_forces_class_balance_which_works_not_well_in_test_set_unfortunately_use_other \\\n",
    "    (features, labels, cv_n_splits = 10, downsampling_factor = 'lowest', l1_log_min = -5, l1_log_max = 2, l1_log_steps = 50):\n",
    "\n",
    "    allowed_print_warning = True\n",
    "    if downsampling_factor == 'lowest':\n",
    "        allowed_print_warning = False\n",
    "        downsampling_factor = 1\n",
    "    \n",
    "    # Estimate number of samples of each class needed.\n",
    "    num_samples_per_class = len(labels) // (2 * downsampling_factor)\n",
    "    \n",
    "    # Get number of samples available for smaller class.\n",
    "    num_positive_samples = np.sum(labels)    \n",
    "    num_samples_smaller_class = min(num_positive_samples, len(labels) - num_positive_samples)    \n",
    "    \n",
    "    if allowed_print_warning and num_samples_smaller_class < num_samples_per_class:\n",
    "        print(f\"[WARNING]: Requested downsampling_factor of {downsampling_factor} with {len(labels)} samples. \" + \\\n",
    "              f\"This would require {num_samples_per_class} samples per class, but minority class only has \" + \\\n",
    "              f\"{num_samples_smaller_class} samples. Using {2 * num_samples_smaller_class} samples to train per \" + \\\n",
    "              f\"class instead.\\n\\n\")\n",
    "    \n",
    "    # Split training samples into balanced classes.    \n",
    "    idx_positive_samples, = np.where(labels == 1)\n",
    "    idx_negative_samples, = np.where(labels == 0)\n",
    "    \n",
    "    idx_positive_samples = np.random.choice(idx_positive_samples, size = num_samples_smaller_class, replace = False)\n",
    "    idx_negative_samples = np.random.choice(idx_negative_samples, size = num_samples_smaller_class, replace = False)\n",
    "\n",
    "    all_idx = list(idx_positive_samples) + list(idx_negative_samples)\n",
    "    \n",
    "    features = features[all_idx]\n",
    "    labels   = labels  [all_idx]\n",
    "    \n",
    "    print(f\"Training on {len(labels)} samples with {features.shape[1]} features\")\n",
    "    \n",
    "    res = GridSearchCV(estimator  = LinearSVC(dual = False, max_iter = 10000), \n",
    "                       param_grid = {'penalty': ('l1', ), 'C': np.logspace(l1_log_min, l1_log_max, l1_log_steps)},\n",
    "                       cv         = StratifiedKFold(cv_n_splits),\n",
    "                       scoring    = {'accuracy' : 'accuracy',\n",
    "                                     'precision': 'precision',\n",
    "                                     'recall'   : 'recall',\n",
    "                                     'f1'       : 'f1',\n",
    "                                     'sparsity' : lambda estimator, X, y: np.mean(np.abs(estimator.coef_) < 1e-8)}, \n",
    "                       return_train_score = True,\n",
    "                       refit = False,\n",
    "                       n_jobs = 8,\n",
    "                       verbose = 2).fit(features, labels)\n",
    "\n",
    "    df = pd.DataFrame(res.cv_results_)\n",
    "    df = df[[col for col in df.columns if col.startswith('param_') or re.match(r'^(mean|std)_(train|test)_', col)]]\n",
    "\n",
    "    # Changing names of sparsity columns and dropping duplicates.\n",
    "    df = df.drop(columns = ['mean_test_sparsity', 'std_test_sparsity']).rename(columns = {'mean_train_sparsity': 'mean_sparsity',\n",
    "                                                                                          'std_train_sparsity' : 'std_sparsity'})\n",
    "    return df, all_idx\n",
    "\n",
    "\n",
    "def plot_columns_over_column(df, x_axis_column, y_axis_columns, x_log_scale = True, figsize = (10, 7)):    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = figsize)\n",
    "    \n",
    "    if x_log_scale:\n",
    "        ax.set(xscale = 'log')\n",
    "    \n",
    "    for y in y_axis_columns:        \n",
    "        ax.errorbar(df[x_axis_column], df[y], df[y.replace('mean', 'std')], label = y.replace('_', ' ').title())\n",
    "        \n",
    "    plt.xlabel(x_axis_column.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f3a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_func(VIRAL_THR, l1_log_min = -5, l1_log_max = 0, resample_balanced = False):\n",
    "    # Seed numpy for reproducibility of all following code.\n",
    "    np.random.seed(0x616461)\n",
    "    \n",
    "    features_train, features_test, num_occurrences_train, num_occurrences_test, viral_label_train, viral_label_test = do_train_test_split(VIRAL_THR)\n",
    "    \n",
    "    df = sweep_l1_regularization(features_train, viral_label_train, \n",
    "                                 cv_n_splits = 10, # Can be reduced down to a minimum of 5 if needed\n",
    "                                 downsampling_factor = 10, # How much to downsample the data. Note that downsampling is stratified,\n",
    "                                                          # so for large values of viral_thr each fold may have very few points.\n",
    "                                                          # This is exactly the issue I had, and why we need to lower viral_thr,\n",
    "                                                          # otherwise models just won't learn.  \n",
    "                                                          # Larger values will make it faster to run, but may lead to worse performance.\n",
    "                                 n_jobs = 8, # This one depends on your CPU and RAM. Higher is faster, but only if you don't fill\n",
    "                                              # your RAM and your CPU is not at 100% all cores. \n",
    "                                              # Start with one, using a fixed downsampling factor and increase it until RAM is filled,\n",
    "                                              # up to a maximum the number of physical cores on your machine.\n",
    "                                              # /!\\ THE COMBINATION OF HIGH N_JOBS AND LOW DOWNSAMPLING FACTOR CAN CRASH YOUR SYSTEM!\n",
    "                                              # (TECHNICALLY ONLY MAKE IT EXTREMELY UNRESPONSIVE, STILL NOT NICE)\n",
    "                                 l1_log_min = l1_log_min, l1_log_max = l1_log_min, l1_log_steps = 20, \n",
    "                                              # Min max and number os steps of sweep logspace.\n",
    "                                              # Min and max probably do not need to be changed.\n",
    "                                              # steps may be changed. Lower makes run faster,\n",
    "                                              # but also decreases resolution of plots.\n",
    "                                              # Warning: due to how regularized SVM \n",
    "                                              # works, large values of C parameter may\n",
    "                                              # lead to very slow convergence (I can provide\n",
    "                                              # intuitive explaination if needed). So for time\n",
    "                                              # reasons it may be intersting to decrease l1_log_max,\n",
    "                                              # FOR AS LONG AS A CONVERGENCE TO A MAX VAL F1 SCORE\n",
    "                                              # CAN STILL BE OBSERVED IN THE PLOTS!!!\n",
    "                                 resample_balanced = resample_balanced) # This one is worth trying: using it for viral_thr\n",
    "                                              # which are deeply unbalanced should allow to achieve better performance.\n",
    "    \n",
    "    # You can see how far in the cv you are in the jupyter console (the black terminal that is opened when you lanch jupyter).\n",
    "    # For some reason, the logs from sklearn CV are written there when n_jobs > 1.\n",
    "    # You can ignore all the UndefinedMetricWarning, unfortunately I was not able to tell sklearn not to show them.\n",
    "    \n",
    "    \n",
    "    pd.options.display.max_rows = 1000\n",
    "    display(df)\n",
    "    \n",
    "    plot_columns_over_column(df, 'param_C', [col for col in df.columns if col.startswith('mean_test_')] + ['mean_sparsity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baad51",
   "metadata": {},
   "source": [
    "# AIM: FIND A VIRAL_THR WHICH TRAINS WELL (GOOD VAL F1 SCORE), ALLOWS TO HAVE GOOD SPARSITY (0.1 SPARSITY WITH NO SIGNIFICANT DROP IN VAL F1 SCORE), AND HAS APPROX <20% OF DATASET WHICH IS A VIRAL QUOTE\n",
    "The importance of each of these parameters can be discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aad43d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viral Thr: 100, 0.399% quotes are viral\n"
     ]
    }
   ],
   "source": [
    "big_func(VIRAL_THR = 100, resample_balanced = False, l1_log_min = -5, l1_log_max = -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_func(VIRAL_THR = 100, resample_balanced = True, l1_log_min = -5, l1_log_max = -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5145d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viral Thr: 5, 9.330% quotes are viral\n",
      "Training on 4834528 samples with 613 features\n",
      "Positive samples: 451047, negative samples: 4383481\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    }
   ],
   "source": [
    "big_func(VIRAL_THR = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064bcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55f81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e17ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c669574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825f885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efb307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(dual = False, max_iter = 10000, penalty = 'l1', C = 0.01).fit(features_train[all_idx], viral_label_train[all_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "105d18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2955fe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03330606282527229"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "f1_score(viral_label_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527de2d8",
   "metadata": {},
   "source": [
    "<a id='training_regression'></a>\n",
    "# 4. Regressing number of occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28843443",
   "metadata": {},
   "source": [
    "<a id='theory_reg'></a>\n",
    "## 4.A. Theoretical explanations of the methods used to perform the regression\n",
    "[Back to table of content](#table_of_contents)\n",
    "\n",
    "In this section, we will describe the mathematical models/concepts of the machine learning methods we will use in order to regress the occurrence of a quote from some features previously selected. \n",
    "\n",
    "We will focus on a simple regression method: linear regression.\n",
    "\n",
    "<a id='lin_reg_theory'></a>\n",
    "### 4.A.a. Linear Regression\n",
    "\n",
    "Regression problems are problems aiming at finding a function $f$ that estimates the relationship between categorical or continuous inputs and an output continuous variable.\n",
    "\n",
    "Linear regression is a specific modelling technique to resolve regression problems. Specifically, it is a model in which we assume that the output variable can be predicted from a linear combination of the input variables such that: \n",
    "\n",
    "$$Y = f(X) = \\alpha_0 + x_1\\alpha_1 ... + x_n\\alpha_n$$\n",
    "\n",
    "Training of the model aims to find the coefficients $\\alpha_i$. To find these, we typically minimize the mean squared error (MSE) defined as follows:\n",
    "\n",
    "$$L_{MSE}(\\alpha; x_i, y_i) = (y_i - f(\\alpha; x_i))^2$$\n",
    "\n",
    "We will also try to add $L2$ or $L1$ regularization terms to push the less meaningful terms towards 0 which will improve the interpretability of the results. In case of the linear regression with no regularization or with $L2$ regularization the solution has a closed-form. For $L1$ regularization there is no closed-form solution. When a closed-form solution is not available, linear regression can be solved via gradient descent. Even if a closed-form solution is available, it should be noted that it may require large amounts of memory to be computed for large training sets.\n",
    "\n",
    "We want to start with linear regression for multiple reasons. The first one is that it is a simple, easy-to-train model which will allow us to handle bigger subsets of the dataset for training in a reasonable time. It's also interesting to see if linear relationships are enough to achieve rather good accuracy for such a complex dataset and it will constitute a good indicator as to whether low bias can be achieved with such a low complexity model.\n",
    "\n",
    "For this model to be acceptable, there are some assumptions:\n",
    "- The relationship between $X$ and $Y$ (features and quote count popularity) are linearly related. This might not be the case but we want to see if it is by gauging the accuracy of the model.\n",
    "\n",
    "- The observations are independent of each other. Of course, this can never truly be the case in such a huge dataset influenced by extremely popular extremely quoted individuals, real-world events, and a ton of other params but we still think that we can apply this model with some amount of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f962e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
