{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73af29f5",
   "metadata": {},
   "source": [
    "## Question:\n",
    "What makes a quote go viral?\n",
    "\n",
    "## Terminology:\n",
    "**VIRAL:** more than 100 occurrences on different sites.\n",
    "\n",
    "## Application:\n",
    "Providing insight on how politicians, influencers, etc. may obtain lots of visibility on a single quote.\n",
    "\n",
    "Providing insight on what classes of people are given more media attention to choose representative of whatever accordingly.\n",
    "\n",
    "## Outcome Variables:\n",
    "- Viral: yes / no\n",
    "- How fast viral viral: in how long viral quotes reached 2/3 of occurrences.\n",
    "\n",
    "## Features:\n",
    "- Indicator variables for 3 most common jobs\n",
    "- Indicator variables for 3 most common genders\n",
    "- Indicator variables for 3 most common ethnicities\n",
    "- Age of speaker\n",
    "- Date of quote (jour + mois + an) in 1 number\n",
    "- Topic: detect most common topics and indicator vars of them\n",
    "\n",
    "## Technique:\n",
    "Linear regression / Logistic regression / SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd792d",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Removal of quotes for which speaker information are not available, as well as quotes from speakers which are not contemporary.\n",
    "Also solve ambiguities in speakers (sometimes several possible speakers possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1548191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec686b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "CACHE_DIR = \"Cache\"\n",
    "SPEAKER_INFO_FILE_PATH = os.path.join(DATA_DIR, \"speaker_attributes.parquet\")\n",
    "PREPROCESSED_DATASET_FILE_PATH = os.path.join(CACHE_DIR, \"preprocessed_dataset.json.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.cache_to_file_pickle(\"function-query_wikidata_for_linkcounts_and_labels\")\n",
    "def query_wikidata_for_linkcounts_and_labels(data_dir, speaker_info_file_path):    \n",
    "    all_speakers = set()\n",
    "    speakers_needing_linkcounts = set()\n",
    "    \n",
    "    for line in utils.all_quotes_generator(data_dir, 1000000):\n",
    "        line_qids_set = set(line['qids'])\n",
    "        \n",
    "        if len(line['qids']) > 1:\n",
    "            speakers_needing_linkcounts |= line_qids_set\n",
    "            \n",
    "        all_speakers |= line_qids_set    \n",
    "        \n",
    "    # Load part of data extracted from Wikidata dump about speakers.\n",
    "    speaker_data = pd.read_parquet(speaker_info_file_path, columns = ['id', 'label', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'candidacy', 'religion'])\n",
    "    \n",
    "    # Immediately remove useless lines to save memory.\n",
    "    speaker_data = speaker_data[speaker_data['id'].isin(all_speakers)]\n",
    "        \n",
    "    # Store id-labels pairs in another variable and remove them from original dataframe.\n",
    "    speaker_qid_labels = speaker_data[['id', 'label']]\n",
    "    speaker_data.drop(columns = ['id', 'label'], inplace = True)\n",
    "        \n",
    "    # Put all qids of informations of all speakers into one single set.\n",
    "    qids_needing_labels = utils.ragged_nested_sequence_to_set(speaker_data.values)\n",
    "    qids_needing_labels.remove(None)\n",
    "        \n",
    "    # Sanity check.\n",
    "    assert all(utils.str_is_qid(qid) for qid in qids_needing_labels)\n",
    "\n",
    "    # Retrieve English labels for informations of all speakers. \n",
    "    qid_labels = utils.get_labels_of_wikidata_ids(ids = qids_needing_labels)\n",
    "    qid_labels = {k: v.title() for k, v in qid_labels.items()}\n",
    "\n",
    "    # Add speakers' id-labels pairs to qid_labels.\n",
    "    speaker_qid_labels = speaker_qid_labels[~speaker_qid_labels.isna().any(axis = 1)].set_index('id').to_dict('index')\n",
    "    speaker_qid_labels = {k: v['label'].title() for k, v in speaker_qid_labels.items()}\n",
    "    qid_labels.update(speaker_qid_labels)\n",
    "    \n",
    "    # Retrieve link counts for speakers for which we need it (used to decide which speaker is most likely being cited\n",
    "    # amongst homonyms).\n",
    "    linkcounts = utils.get_link_counts_of_wikidata_ids(ids = speakers_needing_linkcounts)\n",
    "    linkcounts = {k: int(v) for k, v in linkcounts.items()}\n",
    "\n",
    "    return qid_labels, linkcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 0.422 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 0.868 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 1.295 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 1.705 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 2.123 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 2.544 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 2.967 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2015.json.bz2 in 3.385 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2015.json.bz2 in 3.812 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2015.json.bz2 in 4.224 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2015.json.bz2 in 4.652 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2015.json.bz2 in 5.069 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2015.json.bz2 in 5.484 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2015.json.bz2 in 5.901 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2015.json.bz2 in 6.316 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2015.json.bz2 in 6.730 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2015.json.bz2 in 7.143 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2015.json.bz2 in 7.560 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2015.json.bz2 in 7.994 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2015.json.bz2 in 8.413 minutes\n",
      "Finished processing Data\\quotes-2015.json.bz2 in 8.775 minutes\n",
      "Starting processing Data\\quotes-2016.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2016.json.bz2 in 0.435 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2016.json.bz2 in 0.868 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2016.json.bz2 in 1.300 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2016.json.bz2 in 1.738 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2016.json.bz2 in 2.180 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2016.json.bz2 in 2.620 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2016.json.bz2 in 3.062 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2016.json.bz2 in 3.505 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2016.json.bz2 in 3.953 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2016.json.bz2 in 4.393 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2016.json.bz2 in 4.825 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2016.json.bz2 in 5.262 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2016.json.bz2 in 5.704 minutes\n",
      "Finished processing Data\\quotes-2016.json.bz2 in 6.086 minutes\n",
      "Starting processing Data\\quotes-2017.json.bz2\n"
     ]
    }
   ],
   "source": [
    "qid_labels, linkcounts = query_wikidata_for_linkcounts_and_labels(data_dir = DATA_DIR, speaker_info_file_path = SPEAKER_INFO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47434b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ambiguous_speakers(speakers_qids):   \n",
    "        \n",
    "    # Convert to set to avoid repeating action for same speaker multiple times.\n",
    "    speakers_qids = set(speakers_qids)\n",
    "        \n",
    "    # If there is no ambiguity in the possible speaker qids, return the only possible value.\n",
    "    if len(speakers_qids) == 1:\n",
    "        return speakers_qids.pop()\n",
    "    \n",
    "    return speakers_qids.pop()\n",
    "    \n",
    "    #print(\"INFO: speaker ambiguous\")\n",
    "    \n",
    "    # Recover link counts of each speaker queried from Wikidata. If unavailable, fill with 0.\n",
    "    speakers_linkcounts = {speaker_qid: linkcounts.get(speaker_qid, 0) for speaker_qid in speakers_qids} \n",
    "     \n",
    "    # Return the qid corresponding to the speaker with the largest link count.\n",
    "    return max(speakers_linkcounts, key = speakers_linkcounts.get)\n",
    "\n",
    "\n",
    "def get_speaker_age(birth_date, quote_date, min_age = 5, max_age = 90):\n",
    "    \"\"\"Return param: age: None if speaker too old or an error in dates format encountered.\n",
    "    The value computed for the speaker age otherwise.\"\"\"\n",
    "    \n",
    "    if birth_date is None or quote_date is None:\n",
    "        return\n",
    "        \n",
    "    # CLEVER WAY TO FILTER MOST PROBABLE DATE FROM AMBIGUOUS ONES\n",
    "    birth_date = birth_date[0]\n",
    "\n",
    "    # NEW REGEX WHICH ALLOWS EITHER A PLUS OR NOTHING AT BEGINNING OF STRING, BUT NOT A MINUS! \n",
    "    # (ANYWAY WE DON'T CARE IN THAT CASE AND WE CAN JUST DISCARD STRING AS IF NO MATCH WAS FOUND).\n",
    "    # ALSO ALLOWS FOR A SPACE INSTEAD OF T AND A MISSING Z\n",
    "    date_matcher = getattr(get_speaker_age, 'date_matcher', None)\n",
    "    if date_matcher is None:\n",
    "        date_matcher = re.compile(r\"^[+]?(?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})[T ]\\d{2}:\\d{2}:\\d{2}Z?$\")\n",
    "        get_speaker_age.date_matcher = date_matcher\n",
    "    \n",
    "    birth_date_match = date_matcher.match(birth_date)\n",
    "    if birth_date_match is None:\n",
    "        if birth_date[0] != '-':\n",
    "            print(\"Bad formatted date:\", birth_date)\n",
    "        return\n",
    "    \n",
    "    quote_date_match = date_matcher.match(quote_date)\n",
    "    if quote_date_match is None:\n",
    "        print(\"Bad formatted date:\", quote_date)\n",
    "        return\n",
    "        \n",
    "    birth_year, birth_month, birth_day = birth_date_match.group('year', 'month', 'day')\n",
    "    quote_year, quote_month, quote_day = birth_date_match.group('year', 'month', 'day')\n",
    "    \n",
    "    age = quote_year - birth_year\n",
    "    if quote_month < birth_month or (quote_month == birth_month and day_quote < day_birth):\n",
    "        age -= 1\n",
    "    \n",
    "    return age if min_age <= age <= max_age else None\n",
    "\n",
    "\n",
    "def extract_features(line):\n",
    "    features = {}\n",
    "    \n",
    "    # Extract outcome variable.\n",
    "    features['num_occurrences'] = line['numOccurrences']\n",
    "    \n",
    "    # Extract speaker informations.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract topics of quote.    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract domains fron news urls.\n",
    "    domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "    get_domain_from_url = lambda url: domain_matcher.match(url).group('domain')\n",
    "    features['domains'] = [get_domain_from_url(url) for url in line['urls']]    \n",
    "    \n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_dataset(data_dir, output_file_path, speaker_info_file_path, can_reuse_output = True):\n",
    "    if os.path.isfile(output_file_path) and can_reuse_output:\n",
    "        return\n",
    "    \n",
    "    # Load part of data extracted from Wikidata dump about speakers.\n",
    "    speaker_data = pd.read_parquet(speaker_info_file_path, columns = ['id', 'date_of_birth']).set_index('id').to_dict('index')\n",
    "\n",
    "    with bz2.open(output_file_path, \"wb\") as output_file:\n",
    "        \n",
    "        for line in utils.all_quotes_generator(data_dir):\n",
    "            # Ignore lines for which speaker information is not available.\n",
    "            if not line['qids']:\n",
    "                #print(f\"INFO: speaker unknown: {line['qids']}\\n\")\n",
    "                continue\n",
    "\n",
    "            # Convert list of speaker qids into a single value. If several qids possible, choose the one with largest\n",
    "            # link count.\n",
    "            line['qids'] = solve_ambiguous_speakers(line['qids'])\n",
    "            #line['qids'] = line['qids'][0]\n",
    "\n",
    "            # Try computing age of speaker and ignore lines for which speaker birth date is not available or\n",
    "            # is born too soon to be our contemporary.\n",
    "            speaker_birth_date = speaker_data.get(line['qids'], {}).get('date_of_birth', None)\n",
    "            speaker_age = get_speaker_age(speaker_birth_date, line['date'])\n",
    "            \n",
    "            if speaker_age is None:\n",
    "                #print(f\"INFO: speaker too old: {line['qids']}\\n\")\n",
    "                continue\n",
    "                \n",
    "            # Extract features from line.\n",
    "            #print(f\"INFO: extracting features: {line['qids']}\")\n",
    "            features = extract_features(line)\n",
    "            features['speaker_age'] = speaker_age\n",
    "            \n",
    "            # Store features of line.\n",
    "            output_file.write((json.dumps(features) + '\\n').encode('utf-8'))\n",
    "            #print(f\"INFO: stored features: {line['qids']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 0.680 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 1.380 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 2.062 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 2.741 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 3.765 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 5.415 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 7.104 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7156/2677753649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m preprocess_dataset(DATA_DIR,\n\u001b[0m\u001b[0;32m      2\u001b[0m                    \u001b[0mPREPROCESSED_DATASET_FILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[0mSPEAKER_INFO_FILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    can_reuse_output = False)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7156/4128759837.py\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[1;34m(data_dir, output_file_path, speaker_info_file_path, can_reuse_output)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_quotes_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;31m# Ignore lines for which speaker information is not available.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'qids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ADA\\Project\\utils.py\u001b[0m in \u001b[0;36mall_quotes_generator\u001b[1;34m(data_dir, print_progress_every)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Starting processing {input_file_path}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\bz2.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_can_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\ada\\lib\\_compression.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_dataset(DATA_DIR,\n",
    "                   PREPROCESSED_DATASET_FILE_PATH,\n",
    "                   SPEAKER_INFO_FILE_PATH,\n",
    "                   can_reuse_output = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
