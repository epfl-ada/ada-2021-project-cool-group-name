{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73af29f5",
   "metadata": {},
   "source": [
    "## Question:\n",
    "What makes a quote go viral?\n",
    "\n",
    "## Terminology:\n",
    "**VIRAL:** more than 100 occurrences on different sites.\n",
    "\n",
    "## Application:\n",
    "Providing insight on how politicians, influencers, etc. may obtain lots of visibility on a single quote.\n",
    "\n",
    "Providing insight on what classes of people are given more media attention to choose representative of whatever accordingly.\n",
    "\n",
    "## Outcome Variables:\n",
    "- Viral: yes / no\n",
    "- How fast viral viral: in how long viral quotes reached 2/3 of occurrences.\n",
    "\n",
    "## Features:\n",
    "- Indicator variables for 3 most common jobs\n",
    "- Indicator variables for 3 most common genders\n",
    "- Indicator variables for 3 most common ethnicities\n",
    "- Age of speaker\n",
    "- Date of quote (jour + mois + an) in 1 number\n",
    "- Topic: detect most common topics and indicator vars of them\n",
    "\n",
    "## Technique:\n",
    "Linear regression / Logistic regression / SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c4dcb",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Removal of quotes for which speaker information are not available, as well as quotes from speakers which are not contemporary.\n",
    "Also solve ambiguities in speakers (sometimes several possible speakers possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8faa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b5decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "CACHE_DIR = \"Cache\"\n",
    "SPEAKER_INFO_FILE_PATH = os.path.join(DATA_DIR, \"speaker_attributes.parquet\")\n",
    "PREPROCESSED_DATASET_FILE_PATH = os.path.join(CACHE_DIR, \"preprocessed_dataset.json.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6884c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.cache_to_file_pickle(\"function-query_wikidata_for_linkcounts_and_labels\")\n",
    "def query_wikidata_for_linkcounts_and_labels(data_dir, speaker_info_file_path):    \n",
    "    all_speakers = set()\n",
    "    speakers_needing_linkcounts = set()\n",
    "    \n",
    "    for line in utils.all_quotes_generator(data_dir, 1000000):\n",
    "        line_qids_set = set(line['qids'])\n",
    "        \n",
    "        if len(line['qids']) > 1:\n",
    "            speakers_needing_linkcounts |= line_qids_set\n",
    "            \n",
    "        all_speakers |= line_qids_set    \n",
    "        \n",
    "    # Load part of data extracted from Wikidata dump about speakers.\n",
    "    speaker_data = pd.read_parquet(speaker_info_file_path, columns = ['id', 'label', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'candidacy', 'religion'])\n",
    "    \n",
    "    # Immediately remove useless lines to save memory.\n",
    "    speaker_data = speaker_data[speaker_data['id'].isin(all_speakers)]\n",
    "        \n",
    "    # Store id-labels pairs in another variable and remove them from original dataframe.\n",
    "    speaker_qid_labels = speaker_data[['id', 'label']]\n",
    "    speaker_data.drop(columns = ['id', 'label'], inplace = True)\n",
    "        \n",
    "    # Put all qids of informations of all speakers into one single set.\n",
    "    qids_needing_labels = utils.ragged_nested_sequence_to_set(speaker_data.values)\n",
    "    qids_needing_labels.remove(None)\n",
    "        \n",
    "    # Sanity check.\n",
    "    assert all(utils.str_is_qid(qid) for qid in qids_needing_labels)\n",
    "\n",
    "    # Retrieve English labels for informations of all speakers. \n",
    "    qid_labels = utils.get_labels_of_wikidata_ids(ids = qids_needing_labels)\n",
    "    qid_labels = {k: v.title() for k, v in qid_labels.items()}\n",
    "\n",
    "    # Add speakers' id-labels pairs to qid_labels.\n",
    "    speaker_qid_labels = speaker_qid_labels[~speaker_qid_labels.isna().any(axis = 1)].set_index('id').to_dict('index')\n",
    "    speaker_qid_labels = {k: v['label'].title() for k, v in speaker_qid_labels.items()}\n",
    "    qid_labels.update(speaker_qid_labels)\n",
    "    \n",
    "    # Retrieve link counts for speakers for which we need it (used to decide which speaker is most likely being cited\n",
    "    # amongst homonyms).\n",
    "    linkcounts = utils.get_link_counts_of_wikidata_ids(ids = speakers_needing_linkcounts)\n",
    "    linkcounts = {k: int(v) for k, v in linkcounts.items()}\n",
    "\n",
    "    return qid_labels, linkcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a28761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_labels, linkcounts = query_wikidata_for_linkcounts_and_labels(data_dir = DATA_DIR, speaker_info_file_path = SPEAKER_INFO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92750ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ambiguous_speakers(speakers_qids):   \n",
    "        \n",
    "    # Convert to set to avoid repeating action for same speaker multiple times.\n",
    "    speakers_qids = set(speakers_qids)\n",
    "        \n",
    "    # If there is no ambiguity in the possible speaker qids, return the only possible value.\n",
    "    if len(speakers_qids) == 1:\n",
    "        return speakers_qids.pop()\n",
    "            \n",
    "    # Recover link counts of each speaker queried from Wikidata. If unavailable, fill with 0.\n",
    "    speakers_linkcounts = {speaker_qid: linkcounts.get(speaker_qid, 0) for speaker_qid in speakers_qids} \n",
    "     \n",
    "    # Return the qid corresponding to the speaker with the largest link count.\n",
    "    return max(speakers_linkcounts, key = speakers_linkcounts.get)\n",
    "\n",
    "\n",
    "def get_speaker_age(birth_date, quote_date, min_age = 5, max_age = 90):\n",
    "    \"\"\"Return param: age: None if speaker too old or an error in dates format encountered.\n",
    "    The value computed for the speaker age otherwise.\"\"\"\n",
    "    \n",
    "    if birth_date is None or quote_date is None:\n",
    "        return\n",
    "        \n",
    "    # CLEVER WAY TO FILTER MOST PROBABLE DATE FROM AMBIGUOUS ONES\n",
    "    birth_date = birth_date[0]\n",
    "\n",
    "    # Regular expression matching to year, month and day in dates string in the two used formats. \n",
    "    date_matcher = re.compile(r\"^[+]?(?P<year>-?\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2})[T ]\\d{2}:\\d{2}:\\d{2}Z?$\")\n",
    "    \n",
    "    birth_date_match = date_matcher.match(birth_date)\n",
    "    if birth_date_match is None:\n",
    "        print(\"Bad formatted date:\", birth_date)\n",
    "        return\n",
    "    \n",
    "    quote_date_match = date_matcher.match(quote_date)\n",
    "    if quote_date_match is None:\n",
    "        print(\"Bad formatted date:\", quote_date)\n",
    "        return\n",
    "        \n",
    "    birth_year, birth_month, birth_day = (int(number) for number in birth_date_match.group('year', 'month', 'day'))\n",
    "    quote_year, quote_month, quote_day = (int(number) for number in quote_date_match.group('year', 'month', 'day'))\n",
    "    \n",
    "    age = quote_year - birth_year\n",
    "    if quote_month < birth_month or (quote_month == birth_month and quote_day < birth_day):\n",
    "        age -= 1\n",
    "    \n",
    "    return age if min_age <= age <= max_age else None\n",
    "\n",
    "\n",
    "def extract_features(line):    \n",
    "    features = {}\n",
    "    \n",
    "    # Extract outcome variable.\n",
    "    features['num_occurrences'] = line['numOccurrences']\n",
    "    \n",
    "    # Extract speaker informations.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract topics of quote.    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract domains fron news urls.\n",
    "    domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "    get_domain_from_url = lambda url: domain_matcher.match(url).group('domain')\n",
    "    features['domains'] = [get_domain_from_url(url) for url in line['urls']]    \n",
    "    \n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def preprocess_dataset(data_dir, output_file_path, speaker_info_file_path, can_reuse_output = True):\n",
    "    if os.path.isfile(output_file_path) and can_reuse_output:\n",
    "        return\n",
    "    \n",
    "    # Load part of data extracted from Wikidata dump about speakers.\n",
    "    speaker_data = pd.read_parquet(speaker_info_file_path, columns = ['id', 'date_of_birth']).set_index('id').to_dict('index')\n",
    "\n",
    "    with bz2.open(output_file_path, \"wb\") as output_file:\n",
    "        \n",
    "        for line in utils.all_quotes_generator(data_dir):\n",
    "            # Ignore lines for which speaker information is not available.\n",
    "            if not line['qids']:\n",
    "                continue\n",
    "\n",
    "            # Convert list of speaker qids into a single value.\n",
    "            # If several qids possible, choose the one with largest link count.\n",
    "            line['qids'] = solve_ambiguous_speakers(line['qids'])\n",
    "\n",
    "            # Try computing age of speaker and ignore lines for which speaker birth date is not available or\n",
    "            # is born too soon to be our contemporary.\n",
    "            speaker_birth_date = speaker_data.get(line['qids'], {}).get('date_of_birth', None)\n",
    "            speaker_age = get_speaker_age(speaker_birth_date, line['date'])\n",
    "            \n",
    "            if speaker_age is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract features from line.\n",
    "            features = extract_features(line)\n",
    "            features['speaker_age'] = speaker_age\n",
    "            \n",
    "            # Store features of line.\n",
    "            output_file.write((json.dumps(features) + '\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a38b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing Data\\quotes-2015.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2015.json.bz2 in 0.670 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2015.json.bz2 in 1.337 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2015.json.bz2 in 1.985 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2015.json.bz2 in 2.635 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2015.json.bz2 in 3.273 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2015.json.bz2 in 3.912 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2015.json.bz2 in 4.561 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2015.json.bz2 in 5.203 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2015.json.bz2 in 5.846 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2015.json.bz2 in 6.519 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2015.json.bz2 in 7.182 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2015.json.bz2 in 7.844 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2015.json.bz2 in 8.510 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2015.json.bz2 in 9.171 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2015.json.bz2 in 9.840 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2015.json.bz2 in 10.508 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2015.json.bz2 in 11.176 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2015.json.bz2 in 11.841 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2015.json.bz2 in 12.507 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2015.json.bz2 in 13.172 minutes\n",
      "Finished processing Data\\quotes-2015.json.bz2 in 13.747 minutes\n",
      "Starting processing Data\\quotes-2016.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2016.json.bz2 in 0.696 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2016.json.bz2 in 1.368 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2016.json.bz2 in 2.029 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2016.json.bz2 in 2.693 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2016.json.bz2 in 3.357 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2016.json.bz2 in 4.019 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2016.json.bz2 in 4.684 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2016.json.bz2 in 5.359 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2016.json.bz2 in 6.043 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2016.json.bz2 in 6.734 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2016.json.bz2 in 7.413 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2016.json.bz2 in 8.107 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2016.json.bz2 in 8.796 minutes\n",
      "Finished processing Data\\quotes-2016.json.bz2 in 9.387 minutes\n",
      "Starting processing Data\\quotes-2017.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2017.json.bz2 in 0.785 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2017.json.bz2 in 1.553 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2017.json.bz2 in 2.299 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2017.json.bz2 in 3.034 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2017.json.bz2 in 3.780 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2017.json.bz2 in 4.530 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2017.json.bz2 in 5.272 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2017.json.bz2 in 6.027 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2017.json.bz2 in 6.732 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2017.json.bz2 in 7.462 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2017.json.bz2 in 8.169 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2017.json.bz2 in 8.888 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2017.json.bz2 in 9.600 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2017.json.bz2 in 10.320 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2017.json.bz2 in 11.043 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2017.json.bz2 in 11.805 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2017.json.bz2 in 12.523 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2017.json.bz2 in 13.237 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2017.json.bz2 in 13.959 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2017.json.bz2 in 14.684 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2017.json.bz2 in 15.401 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2017.json.bz2 in 16.118 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2017.json.bz2 in 16.838 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2017.json.bz2 in 17.558 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2017.json.bz2 in 18.267 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2017.json.bz2 in 18.980 minutes\n",
      "Finished processing Data\\quotes-2017.json.bz2 in 19.425 minutes\n",
      "Starting processing Data\\quotes-2018.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2018.json.bz2 in 0.686 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2018.json.bz2 in 1.368 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2018.json.bz2 in 2.051 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2018.json.bz2 in 2.734 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2018.json.bz2 in 3.412 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2018.json.bz2 in 4.094 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2018.json.bz2 in 4.775 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2018.json.bz2 in 5.449 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2018.json.bz2 in 6.135 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2018.json.bz2 in 6.817 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2018.json.bz2 in 7.503 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2018.json.bz2 in 8.184 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2018.json.bz2 in 8.870 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2018.json.bz2 in 9.553 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2018.json.bz2 in 10.232 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2018.json.bz2 in 10.913 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2018.json.bz2 in 11.589 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2018.json.bz2 in 12.269 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2018.json.bz2 in 12.946 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2018.json.bz2 in 13.617 minutes\n",
      "Processed 21000000 lines from Data\\quotes-2018.json.bz2 in 14.297 minutes\n",
      "Processed 22000000 lines from Data\\quotes-2018.json.bz2 in 14.976 minutes\n",
      "Processed 23000000 lines from Data\\quotes-2018.json.bz2 in 15.655 minutes\n",
      "Processed 24000000 lines from Data\\quotes-2018.json.bz2 in 16.362 minutes\n",
      "Processed 25000000 lines from Data\\quotes-2018.json.bz2 in 17.060 minutes\n",
      "Processed 26000000 lines from Data\\quotes-2018.json.bz2 in 17.742 minutes\n",
      "Processed 27000000 lines from Data\\quotes-2018.json.bz2 in 18.431 minutes\n",
      "Finished processing Data\\quotes-2018.json.bz2 in 18.587 minutes\n",
      "Starting processing Data\\quotes-2019.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2019.json.bz2 in 0.661 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2019.json.bz2 in 1.320 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2019.json.bz2 in 1.980 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2019.json.bz2 in 2.641 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2019.json.bz2 in 3.300 minutes\n",
      "Processed 6000000 lines from Data\\quotes-2019.json.bz2 in 3.970 minutes\n",
      "Processed 7000000 lines from Data\\quotes-2019.json.bz2 in 4.631 minutes\n",
      "Processed 8000000 lines from Data\\quotes-2019.json.bz2 in 5.291 minutes\n",
      "Processed 9000000 lines from Data\\quotes-2019.json.bz2 in 5.949 minutes\n",
      "Processed 10000000 lines from Data\\quotes-2019.json.bz2 in 6.604 minutes\n",
      "Processed 11000000 lines from Data\\quotes-2019.json.bz2 in 7.260 minutes\n",
      "Processed 12000000 lines from Data\\quotes-2019.json.bz2 in 7.918 minutes\n",
      "Processed 13000000 lines from Data\\quotes-2019.json.bz2 in 8.572 minutes\n",
      "Processed 14000000 lines from Data\\quotes-2019.json.bz2 in 9.232 minutes\n",
      "Processed 15000000 lines from Data\\quotes-2019.json.bz2 in 9.890 minutes\n",
      "Processed 16000000 lines from Data\\quotes-2019.json.bz2 in 10.538 minutes\n",
      "Processed 17000000 lines from Data\\quotes-2019.json.bz2 in 11.198 minutes\n",
      "Processed 18000000 lines from Data\\quotes-2019.json.bz2 in 11.858 minutes\n",
      "Processed 19000000 lines from Data\\quotes-2019.json.bz2 in 12.516 minutes\n",
      "Processed 20000000 lines from Data\\quotes-2019.json.bz2 in 13.174 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 21000000 lines from Data\\quotes-2019.json.bz2 in 13.838 minutes\n",
      "Finished processing Data\\quotes-2019.json.bz2 in 14.342 minutes\n",
      "Starting processing Data\\quotes-2020.json.bz2\n",
      "Processed 1000000 lines from Data\\quotes-2020.json.bz2 in 0.655 minutes\n",
      "Processed 2000000 lines from Data\\quotes-2020.json.bz2 in 1.305 minutes\n",
      "Processed 3000000 lines from Data\\quotes-2020.json.bz2 in 1.954 minutes\n",
      "Processed 4000000 lines from Data\\quotes-2020.json.bz2 in 2.602 minutes\n",
      "Processed 5000000 lines from Data\\quotes-2020.json.bz2 in 3.252 minutes\n",
      "Finished processing Data\\quotes-2020.json.bz2 in 3.410 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68579656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset(DATA_DIR,\n",
    "                   PREPROCESSED_DATASET_FILE_PATH,\n",
    "                   SPEAKER_INFO_FILE_PATH,\n",
    "                   can_reuse_output = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
