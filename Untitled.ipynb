{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a709e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import utils\n",
    "from collections import Counter\n",
    "import bz2\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96d7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, data_dir, extension = '.json.bz2', lines = True, chunksize = 50000, **read_kwargs):\n",
    "        filenames = [filename for filename in os.listdir(data_dir) if filename.endswith(extension)]\n",
    "        self.files_paths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "\n",
    "        self.read_kwargs = read_kwargs\n",
    "        self.read_kwargs['lines'] = lines\n",
    "        self.read_kwargs['chunksize'] = chunksize\n",
    "        \n",
    "    def load(self):\n",
    "        data = {}\n",
    "        \n",
    "        for file_path in self.files_paths:\n",
    "            start = time.start()\n",
    "            \n",
    "            speakers_data_file = self._load_one_file(file_path = file_path,\n",
    "                                                     read_kwargs = self.read_kwargs)\n",
    "            \n",
    "            speakers_data = self._count_unique_speakers_merge_series(speakers_data_file, speakers_data)                \n",
    "            print(\"Elapsed:\", time.time() - start, \"seconds\")\n",
    "            \n",
    "        return pd.DataFrame(speakers_data)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    @utils.cache_to_file_pickle(\"DataLoader-_load_one_file\", ignore_kwargs = [\"read_kwargs\"])\n",
    "    def _load_one_file(file_path, read_kwargs):\n",
    "        data = []\n",
    "\n",
    "        for block in pd.io.json.read_json(file_path, **read_kwargs):\n",
    "            block = block[['quotation', 'speaker', 'numOccurrences', 'date', 'qids', 'urls']]\n",
    "            \n",
    "            data_block = {}\n",
    "            \n",
    "            \n",
    "            grouped_block = block.groupby('speaker')\n",
    "            aggregated_block = grouped_block.aggregate({'numOccurrences': 'sum', 'qids': 'sum'})\n",
    "\n",
    "            speakers_data = DataLoader._count_unique_speakers_merge_series({'numOccurrences': aggregated_block['numOccurrences'],\n",
    "                                                                            'qids': aggregated_block['qids'],\n",
    "                                                                            'numQuotes': grouped_block.size()},\n",
    "\n",
    "                                                                           {'numOccurrences': speakers_data['numOccurrences'],\n",
    "                                                                            'qids': speakers_data['qids'],\n",
    "                                                                            'numQuotes': speakers_data['numQuotes']})\n",
    "\n",
    "            print(\"Read block\")\n",
    "                \n",
    "            \n",
    "        return speakers_data\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def count_unique_speakers(self):\n",
    "        speakers_data = {'numOccurrences': pd.Series(dtype = int), \n",
    "                         'numQuotes': pd.Series(dtype = int),\n",
    "                         'qids': pd.Series(dtype = int)}\n",
    "                                    \n",
    "        for file_path in self.files_paths:\n",
    "            start = time.time()\n",
    "            \n",
    "            speakers_data_file = self._count_unique_speakers_one_file(file_path = file_path,\n",
    "                                                                      read_kwargs = self.read_kwargs)\n",
    "            \n",
    "            speakers_data = self._count_unique_speakers_merge_series(speakers_data_file, speakers_data)                \n",
    "            print(\"Elapsed:\", time.time() - start, \"seconds\")\n",
    "            \n",
    "        return pd.DataFrame(speakers_data)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _count_unique_speakers_merge_series(series_dict1, series_dict2):\n",
    "        merged_series = {}\n",
    "        merged_series['numOccurrences'] = series_dict1['numOccurrences'].add(series_dict2['numOccurrences'], fill_value = 0)\n",
    "        merged_series['numQuotes'] = series_dict1['numQuotes'].add(series_dict2['numQuotes'], fill_value = 0)\n",
    "        merged_series['qids'] = series_dict1['qids'].combine(series_dict2['qids'], lambda x, y: set(x).union(y), fill_value = set())\n",
    "        return merged_series\n",
    "    \n",
    "    @staticmethod\n",
    "    @utils.cache_to_file_pickle(\"DataLoader-_count_unique_speakers_one_file\")\n",
    "    def _count_unique_speakers_one_file(file_path, read_kwargs):\n",
    "        speakers_data = {'numOccurrences': pd.Series(dtype = int), \n",
    "                         'numQuotes': pd.Series(dtype = int),\n",
    "                         'qids': pd.Series(dtype = int)}\n",
    "\n",
    "        for block in pd.io.json.read_json(file_path, **read_kwargs):\n",
    "            block = block[['speaker', 'numOccurrences', 'qids']]\n",
    "            grouped_block = block.groupby('speaker')\n",
    "            aggregated_block = grouped_block.aggregate({'numOccurrences': 'sum', 'qids': 'sum'})\n",
    "\n",
    "            speakers_data = DataLoader._count_unique_speakers_merge_series({'numOccurrences': aggregated_block['numOccurrences'],\n",
    "                                                                            'qids': aggregated_block['qids'],\n",
    "                                                                            'numQuotes': grouped_block.size()},\n",
    "\n",
    "                                                                           {'numOccurrences': speakers_data['numOccurrences'],\n",
    "                                                                            'qids': speakers_data['qids'],\n",
    "                                                                            'numQuotes': speakers_data['numQuotes']})\n",
    "\n",
    "            print(\"Read block\")\n",
    "                \n",
    "            \n",
    "        return speakers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715be80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, data_dir, extension = '.json.bz2'):\n",
    "        filenames = [filename for filename in os.listdir(data_dir) if filename.endswith(extension)]\n",
    "        self.files_paths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "        \n",
    "    def load(self):\n",
    "        data = {key: [] for key in ['quote_word_count', 'speaker', 'qids', 'first_date', 'n_occurrences', 'domains']}\n",
    "        \n",
    "        for file_path in self.files_paths:\n",
    "            start = time.time()\n",
    "            \n",
    "            data_one_file = self._load_one_file(file_path = file_path)\n",
    "            \n",
    "            # Extend overall list with elements from current file and free memory at the same time.\n",
    "            for key in data.keys():\n",
    "                data[key].extend(data_one_file.pop(key))\n",
    "            \n",
    "            print(\"Elapsed:\", time.time() - start, \"seconds\")\n",
    "            \n",
    "        return pd.DataFrame.from_dict(data)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    @utils.cache_to_file_pickle(\"DataLoader-_load_one_file\")\n",
    "    def _load_one_file(file_path):       \n",
    "        data = {key: [] for key in ['quote_word_count', 'speaker', 'qids', 'first_date', 'n_occurrences', 'domains']}\n",
    "        \n",
    "        domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\")\n",
    "        get_domain_from_url = lambda string: domain_matcher.match(string).group(1)\n",
    "            \n",
    "        with bz2.open(file_path, 'rb') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                line = json.loads(line)\n",
    "                \n",
    "                data['quote_word_count'].append(Counter(remove_punctuation(line['quotation']).split()))\n",
    "                data['speaker']         .append(line['speaker'])\n",
    "                data['qids']            .append(set(line['qids']))\n",
    "                data['first_date']      .append(pd.to_datetime(line['date']))\n",
    "                data['newspapers']      .append([get_domain_from_url(url) for url in line['urls']])\n",
    "                                                \n",
    "                if not i % 1000000:\n",
    "                    print(\"Read\", i, \"lines\")\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(data_dir, output_file_path, can_reuse_output = True):\n",
    "    if os.path.isfile(output_file_path) and can_reuse_output:\n",
    "        return\n",
    "    \n",
    "    filenames = [filename for filename in os.listdir(data_dir) if filename.endswith('.json.bz2')]\n",
    "    input_files_paths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "\n",
    "    domain_matcher = re.compile(r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?(?P<domain>[^:\\/?\\n]+)\")\n",
    "    get_domain_from_url = lambda string: domain_matcher.match(string).group('domain')\n",
    "        \n",
    "    with bz2.open(output_file_path, 'wb') as output_file:\n",
    "        for input_file_path in input_files_paths:\n",
    "            start = time.time()\n",
    "            \n",
    "            with bz2.open(input_file_path, 'rb') as input_file:\n",
    "                for i, line in enumerate(input_file):\n",
    "                    line = json.loads(line)\n",
    "                    \n",
    "                    data_line = {'quote_word_count': Counter(line['quotation']),\n",
    "                                 'speaker'         : line['speaker'],\n",
    "                                 'qids'            : line['qids'],\n",
    "                                 'first_date'      : line['date'],\n",
    "                                 'domains'         : [get_domain_from_url(url) for url in line['urls']]}\n",
    "\n",
    "                    output_file.write((json.dumps(data_line) + '\\n').encode('utf-8'))\n",
    "                    \n",
    "                    if not i % 1000000:\n",
    "                        print(\"Read\", i, \"lines from\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")\n",
    "                        \n",
    "            print(\"Finished reading\", input_file_path, 'in', (time.time() - start) / 60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 lines from Data\\quotes-2015.json.bz2 in 0.00021671454111735027 minutes\n",
      "Read 1000000 lines from Data\\quotes-2015.json.bz2 in 1.2587339917818705 minutes\n",
      "Read 2000000 lines from Data\\quotes-2015.json.bz2 in 2.5700877865155536 minutes\n",
      "Read 3000000 lines from Data\\quotes-2015.json.bz2 in 3.8852311611175536 minutes\n",
      "Read 4000000 lines from Data\\quotes-2015.json.bz2 in 5.177006955941518 minutes\n",
      "Read 5000000 lines from Data\\quotes-2015.json.bz2 in 6.536114553610484 minutes\n",
      "Read 6000000 lines from Data\\quotes-2015.json.bz2 in 7.855174728234609 minutes\n",
      "Read 7000000 lines from Data\\quotes-2015.json.bz2 in 9.169197348753611 minutes\n",
      "Read 8000000 lines from Data\\quotes-2015.json.bz2 in 10.495114398002624 minutes\n",
      "Read 9000000 lines from Data\\quotes-2015.json.bz2 in 11.803927584489186 minutes\n",
      "Read 10000000 lines from Data\\quotes-2015.json.bz2 in 13.157017437616984 minutes\n",
      "Read 11000000 lines from Data\\quotes-2015.json.bz2 in 14.450017511844635 minutes\n",
      "Read 12000000 lines from Data\\quotes-2015.json.bz2 in 15.773671384652456 minutes\n",
      "Read 13000000 lines from Data\\quotes-2015.json.bz2 in 17.09075138171514 minutes\n",
      "Read 14000000 lines from Data\\quotes-2015.json.bz2 in 18.41524084806442 minutes\n",
      "Read 15000000 lines from Data\\quotes-2015.json.bz2 in 19.688152078787486 minutes\n",
      "Read 16000000 lines from Data\\quotes-2015.json.bz2 in 20.945383735497792 minutes\n",
      "Read 17000000 lines from Data\\quotes-2015.json.bz2 in 22.19255535999934 minutes\n",
      "Read 18000000 lines from Data\\quotes-2015.json.bz2 in 23.486497219403585 minutes\n",
      "Read 19000000 lines from Data\\quotes-2015.json.bz2 in 24.752881185213724 minutes\n",
      "Read 20000000 lines from Data\\quotes-2015.json.bz2 in 26.017873895168304 minutes\n",
      "Finished reading Data\\quotes-2015.json.bz2 in 27.122554818789165 minutes\n",
      "Read 0 lines from Data\\quotes-2016.json.bz2 in 0.00023355484008789063 minutes\n",
      "Read 1000000 lines from Data\\quotes-2016.json.bz2 in 1.3530835350354513 minutes\n",
      "Read 2000000 lines from Data\\quotes-2016.json.bz2 in 2.705693976084391 minutes\n",
      "Read 3000000 lines from Data\\quotes-2016.json.bz2 in 4.033694819609324 minutes\n",
      "Read 4000000 lines from Data\\quotes-2016.json.bz2 in 5.37942164738973 minutes\n",
      "Read 5000000 lines from Data\\quotes-2016.json.bz2 in 6.688968352476755 minutes\n",
      "Read 6000000 lines from Data\\quotes-2016.json.bz2 in 7.998285019397736 minutes\n",
      "Read 7000000 lines from Data\\quotes-2016.json.bz2 in 9.309385228157044 minutes\n",
      "Read 8000000 lines from Data\\quotes-2016.json.bz2 in 10.610953879356384 minutes\n",
      "Read 9000000 lines from Data\\quotes-2016.json.bz2 in 11.93249595562617 minutes\n",
      "Read 10000000 lines from Data\\quotes-2016.json.bz2 in 13.291707615057627 minutes\n",
      "Read 11000000 lines from Data\\quotes-2016.json.bz2 in 14.63658592303594 minutes\n",
      "Read 12000000 lines from Data\\quotes-2016.json.bz2 in 15.936046997706095 minutes\n",
      "Read 13000000 lines from Data\\quotes-2016.json.bz2 in 17.231140406926475 minutes\n",
      "Finished reading Data\\quotes-2016.json.bz2 in 18.35211611588796 minutes\n",
      "Read 0 lines from Data\\quotes-2017.json.bz2 in 0.00021669467290242513 minutes\n",
      "Read 1000000 lines from Data\\quotes-2017.json.bz2 in 1.5940999706586203 minutes\n",
      "Read 2000000 lines from Data\\quotes-2017.json.bz2 in 3.135893468062083 minutes\n",
      "Read 3000000 lines from Data\\quotes-2017.json.bz2 in 4.624630502859751 minutes\n",
      "Read 4000000 lines from Data\\quotes-2017.json.bz2 in 6.117608431975047 minutes\n",
      "Read 5000000 lines from Data\\quotes-2017.json.bz2 in 7.632768400510153 minutes\n",
      "Read 6000000 lines from Data\\quotes-2017.json.bz2 in 9.156334535280864 minutes\n",
      "Read 7000000 lines from Data\\quotes-2017.json.bz2 in 10.685231975714366 minutes\n",
      "Read 8000000 lines from Data\\quotes-2017.json.bz2 in 12.332395513852438 minutes\n",
      "Read 9000000 lines from Data\\quotes-2017.json.bz2 in 13.913570396105449 minutes\n",
      "Read 10000000 lines from Data\\quotes-2017.json.bz2 in 15.492685608069102 minutes\n",
      "Read 11000000 lines from Data\\quotes-2017.json.bz2 in 17.18540242513021 minutes\n",
      "Read 12000000 lines from Data\\quotes-2017.json.bz2 in 18.88107806444168 minutes\n",
      "Read 13000000 lines from Data\\quotes-2017.json.bz2 in 20.46753747065862 minutes\n",
      "Read 14000000 lines from Data\\quotes-2017.json.bz2 in 22.066428387165068 minutes\n",
      "Read 15000000 lines from Data\\quotes-2017.json.bz2 in 23.718931206067403 minutes\n",
      "Read 16000000 lines from Data\\quotes-2017.json.bz2 in 25.42715943257014 minutes\n",
      "Read 17000000 lines from Data\\quotes-2017.json.bz2 in 27.02153731584549 minutes\n",
      "Read 18000000 lines from Data\\quotes-2017.json.bz2 in 28.655076456069946 minutes\n",
      "Read 19000000 lines from Data\\quotes-2017.json.bz2 in 30.29956568876902 minutes\n",
      "Read 20000000 lines from Data\\quotes-2017.json.bz2 in 31.995466554164885 minutes\n",
      "Read 21000000 lines from Data\\quotes-2017.json.bz2 in 33.6274853070577 minutes\n",
      "Read 22000000 lines from Data\\quotes-2017.json.bz2 in 35.283777511119844 minutes\n",
      "Read 23000000 lines from Data\\quotes-2017.json.bz2 in 36.923731072743735 minutes\n",
      "Read 24000000 lines from Data\\quotes-2017.json.bz2 in 38.595443137486775 minutes\n",
      "Read 25000000 lines from Data\\quotes-2017.json.bz2 in 40.26608828703562 minutes\n",
      "Read 26000000 lines from Data\\quotes-2017.json.bz2 in 41.94209768772125 minutes\n",
      "Finished reading Data\\quotes-2017.json.bz2 in 43.02861049572627 minutes\n",
      "Read 0 lines from Data\\quotes-2018.json.bz2 in 3.073832392692566 minutes\n",
      "Read 1000000 lines from Data\\quotes-2018.json.bz2 in 4.492072840531667 minutes\n",
      "Read 2000000 lines from Data\\quotes-2018.json.bz2 in 5.863298229376475 minutes\n",
      "Read 3000000 lines from Data\\quotes-2018.json.bz2 in 7.230828277269999 minutes\n",
      "Read 4000000 lines from Data\\quotes-2018.json.bz2 in 8.595313294728596 minutes\n",
      "Read 5000000 lines from Data\\quotes-2018.json.bz2 in 9.957771400610605 minutes\n",
      "Read 6000000 lines from Data\\quotes-2018.json.bz2 in 11.326088043053945 minutes\n",
      "Read 7000000 lines from Data\\quotes-2018.json.bz2 in 12.687541083494823 minutes\n",
      "Read 8000000 lines from Data\\quotes-2018.json.bz2 in 14.061160957813263 minutes\n",
      "Read 9000000 lines from Data\\quotes-2018.json.bz2 in 15.438029539585113 minutes\n",
      "Read 10000000 lines from Data\\quotes-2018.json.bz2 in 16.833911657333374 minutes\n",
      "Read 11000000 lines from Data\\quotes-2018.json.bz2 in 18.224627339839934 minutes\n",
      "Read 12000000 lines from Data\\quotes-2018.json.bz2 in 19.658143428961434 minutes\n",
      "Read 13000000 lines from Data\\quotes-2018.json.bz2 in 21.1059290210406 minutes\n",
      "Read 14000000 lines from Data\\quotes-2018.json.bz2 in 22.509984481334687 minutes\n",
      "Read 15000000 lines from Data\\quotes-2018.json.bz2 in 23.93819682598114 minutes\n",
      "Read 16000000 lines from Data\\quotes-2018.json.bz2 in 25.393041825294496 minutes\n",
      "Read 17000000 lines from Data\\quotes-2018.json.bz2 in 26.85428952773412 minutes\n",
      "Read 18000000 lines from Data\\quotes-2018.json.bz2 in 28.311674666404723 minutes\n",
      "Read 19000000 lines from Data\\quotes-2018.json.bz2 in 29.739038717746734 minutes\n",
      "Read 20000000 lines from Data\\quotes-2018.json.bz2 in 31.1459645430247 minutes\n",
      "Read 21000000 lines from Data\\quotes-2018.json.bz2 in 32.587529095013934 minutes\n",
      "Read 22000000 lines from Data\\quotes-2018.json.bz2 in 34.036913951238 minutes\n",
      "Read 23000000 lines from Data\\quotes-2018.json.bz2 in 35.476040621598564 minutes\n",
      "Read 24000000 lines from Data\\quotes-2018.json.bz2 in 36.90320820808411 minutes\n",
      "Read 25000000 lines from Data\\quotes-2018.json.bz2 in 38.351695811748506 minutes\n",
      "Read 26000000 lines from Data\\quotes-2018.json.bz2 in 39.778276391824086 minutes\n",
      "Read 27000000 lines from Data\\quotes-2018.json.bz2 in 41.21915084520976 minutes\n",
      "Finished reading Data\\quotes-2018.json.bz2 in 41.54415780703227 minutes\n",
      "Read 0 lines from Data\\quotes-2019.json.bz2 in 3.1545841614405314 minutes\n",
      "Read 1000000 lines from Data\\quotes-2019.json.bz2 in 4.562652945518494 minutes\n",
      "Read 2000000 lines from Data\\quotes-2019.json.bz2 in 5.96176837682724 minutes\n",
      "Read 3000000 lines from Data\\quotes-2019.json.bz2 in 7.352112364768982 minutes\n",
      "Read 4000000 lines from Data\\quotes-2019.json.bz2 in 8.731046255429586 minutes\n",
      "Read 5000000 lines from Data\\quotes-2019.json.bz2 in 10.077807760238647 minutes\n",
      "Read 6000000 lines from Data\\quotes-2019.json.bz2 in 11.443913420041403 minutes\n",
      "Read 7000000 lines from Data\\quotes-2019.json.bz2 in 12.802527681986492 minutes\n",
      "Read 8000000 lines from Data\\quotes-2019.json.bz2 in 14.159415292739869 minutes\n",
      "Read 9000000 lines from Data\\quotes-2019.json.bz2 in 15.513778320948283 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10000000 lines from Data\\quotes-2019.json.bz2 in 16.892230383555095 minutes\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'Data'\n",
    "CACHE_DIR = 'Cache'\n",
    "\n",
    "process_files(DATA_DIR, os.path.join(CACHE_DIR, 'processed_data.json.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f930f",
   "metadata": {},
   "source": [
    "# POUR KAOU\n",
    "\n",
    "Bonjour Kaou, tu as choisi word counting. Malheureusement je me suis rendu trop tard dans l'execution de la cellule précédente que j'avais oublié d'enlever les punctuation. Oups. Tu vas devoir itérer sur tous les clés du Compteur et enlever manuellement, et merge aussi les mêmes mots. Ou juste relancer la cellule du haut que j'ai déjà corrigé, mais que je n'ai pas eu le courage de relancer parce que ça prend beaucoup trop de temps (mais pas de resources, donc ne te fais pas de soucis tu peux le run sur n'importe quoi).\n",
    "\n",
    "Lis cette page pour des idées sur comment faire évoluer ton travail au délà du word counting: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Je te mets aussi à disposition une simple fonction qui efface les \"English stop words\" d'une liste de mots. Par contre, fais gaffe à comment tu l'utilises. Les stop words ne sont pas nécéssairement toutes toujours inutiles. Lis sklearn pour les détails et affiche la liste avant d'utiliser la fonction pour voir si elle t'arrange. Tu peux covertir la fonction pour qu'elle droppe le compte des stop words dans un compteur en faisant del counter_object[word]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d003a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    return re.sub(r'[^a-z0-9 ]', '', string)\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    return [word for word in word_list if word not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c671f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0e7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd282f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a01fe7",
   "metadata": {},
   "source": [
    "# POUR CÉLINA\n",
    "\n",
    "Coucou CC, il nous ont donné un file .parquet (juste un autre format de stockage binaire, un peux comme pickle) que normalement j'ai mis dans le dossier Data. Il devrait contenir plusieures informations sur chaque speaker. Tu peux le load comme ça, mais il me semble qu'il faut installer pyarrow (  conda install -c conda-forge pyarrow  ).\n",
    "\n",
    "Malheureusement il parait qu'ils aient utilisé le QID de Wikidata pour encoder la profession, religion et tout le reste. Probablement une des première choses à faire est de trouver un moyen de mapper les QIDS à des string, que je sais faire en faisant des queries à wikidata mais je me demande s'il n'y a pas un moyen plus simple. Au pire on le fait (que une fois de toute façon).\n",
    "\n",
    "Il faudrait aussi s'assurer que dans le parquet qu'ils nous donnent il n'y ait pas que peu d'occupations au bol, mais un grand nombre comme celui qu'on voit sur wikidata, car autrement on a le même problème que quand je faisais les queries (que j'ai découvert comment resudre) qui est que pour des gens avec beaucoup de professions on en avait seulement 3 au bol.\n",
    "\n",
    "Pour le moment c'est tout je crois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Data/speaker_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e891fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ce619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4a5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb740841",
   "metadata": {},
   "source": [
    "# POUR ANDREA\n",
    "Bonjour Andrea, ça va? Oui très bien, merci. J'essaye de poser une base pour commencer le Milestone 2 du projet. Et toi? Moi aussi, drôle ça. Bon, à toute. Bon travail, à toute.\n",
    "\n",
    "Il faut rerun la lecture du dataset avec la remotion de ponctuation.\n",
    "\n",
    "Je suppose que tu vas faire la partie de correler les dates à des événements, et si t'as envie d'essayer d'extraire la variance. Par contre pour le moment je ne sais pas trop comment ça colle avec le reste de l'analyse. Dans le sens que la data story va parler de quoi concernant les dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d3e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa53452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5505d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f277e21e",
   "metadata": {},
   "source": [
    "# POUR MATTIA\n",
    "Buongiorno Mattia, je suppose que tu vas faire la partie de regarder les newspapers et correler avec les speakers et, si on arrive, l'argument de la quote. Pour toi je crois que juste load le processed_data.json.bz2 et garder que les clés 'domains' et 'speaker' devrait le faire. Il faudra sûrement se coordonner avec Célina et Kaou pour voir justement comment correler les trucs. Pour le moment, je t'avoue que comme pour ma partie, je n'ai pas une idéé précide de comment ceci va coller dans une data story coherente. Faudra que Kaou et Célina avancent rapidement pour que tu puisse commencer à repliquer leur travail mais pour différents newspapers. Ou je ne sais pas. Faudra juste pas qu'on reste bloqués si on attend quelqu'un d'autre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
