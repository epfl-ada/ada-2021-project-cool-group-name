import time
import numpy as np
import scipy.stats
import scipy.sparse
from copy import deepcopy
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import *

import src.utils as utils

    
def _folds_generator(X, y, cv_n_splits, problem_type, verbose, train_on_single_fold = False):
    """
    Function yielding folds from X and y. The number of folds is given by cv_n_splits and if problem type is
    'regression' then the folds are generated by stratifying with a percentile stratifier, whereas if they 
    problem type is 'classification' then the folds are generated by stratifying with the binary labels.
    If train on single fold is True, the training set each fold will be composed of a single one of the K folds,
    while the validation set will be made of all remaining ones. 
    
    Params:
        X::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        y::[np.array]
            Array containing the training outcomes (or labels) corresponding to samples in X.
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        problem_type::[str]
            'regression' if this function is used as folds generator for a regression problem and 'classification'
            if it is used for a classification problem.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
        train_on_single_fold::[bool]
            Wether to use a single fold for training and cv_n_splits-1 folds for validation or the other way round.
            
    Yields:
        X_train::[np.array | scipy.sparse.matrix]
            Matrix containing the training samples to use for the current fold.
        X_val::[np.array | scipy.sparse.matrix]
            Matrix containing the validation samples to use for the current fold.
        y_train::[np.array]
            Matrix containing the training outcomes to use for the current fold.
        y_val::[np.array]
            Matrix containing the validation outcomes to use for the current fold.
        
    Returns:
        None
    """
    stratified_k_folds = StratifiedKFold(n_splits = cv_n_splits, shuffle = True)
    
    if problem_type == 'regression':
        folds_idx = stratified_k_folds.split(np.zeros_like(y), utils.get_percentile_stratifier(y))
    
    elif problem_type == 'classification':
        folds_idx = stratified_k_folds.split(np.zeros_like(y), y)

    else:
        raise ValueError("Parameter 'problem_type' must be either 'regression' or 'classification'.")
    
    # Generator to list.
    folds_idx = list(folds_idx)
    
    if train_on_single_fold:
        folds_idx = [elem[::-1] for elem in folds_idx]
    
    for i, (train_idx, val_idx) in enumerate(folds_idx):
        if verbose:
            print(f"Starting fold {i + 1} of {len(folds_idx)}.")
            print(f"Fold contains {len(train_idx)} training samples and {len(val_idx)} validation samples.")
            start = time.time()
        
        yield X[train_idx], X[val_idx], y[train_idx], y[val_idx]
        
        if verbose:
            print(f"Finished Fold {i + 1} of {len(folds_idx)} in {int(time.time() - start)} seconds.")


def _scores_regression(y, preds):
    """
    Function computing the scores used to quantify performance of a regressor. 
    
    Params:
        y::[np.array]
            The true outcome.
        preds::[np.array]
            The outcome predicted by the regressor.
        
    Returns:
        results::[dict]
            Dictionary containing keys 'r2', 'mse', 'explained_variance', 'max_error', 'median_absolute_error'
            and as values the corresponding regression score computed from true and predicted outcomes.
    """
    return {'r2'                   : r2_score(y, preds),
            'mse'                  : mean_squared_error(y, preds),
            'explained_variance'   : explained_variance_score(y, preds),
            'max_error'            : max_error(y, preds),
            'median_absolute_error': median_absolute_error(y, preds)}           


def _scores_classification(y, preds):
    """
    Function computing the scores used to quantify performance of a classifier. 
    
    Params:
        y::[np.array]
            The true labels.
        preds::[np.array]
            The labels predicted by the classifier.
        
    Returns:
        results::[dict]
            Dictionary containing keys 'accuracy', 'precision', 'recall', 'f1' and as values the corresponding 
            classification score computed from true and predicted labels.
    """
    return {'accuracy'  : accuracy_score(y, preds),
            'precision' : precision_score(y, preds),
            'recall'    : recall_score(y, preds),
            'f1'        : f1_score(y, preds)}


@utils.cache_to_file_pickle("cross_validation-baseline_regression_cv", ignore_kwargs = ['features', 'verbose'])
def baseline_regression_cv(features, num_occurrences, features_cols_titles, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold.
    The regressor used by this function always predicts the mean.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        num_occurrences::[np.array]
            Array containing the training outcomes corresponding to samples in features.
        features_cols_titles::[list | tuple]
            List of strings containing the name assigned to each column in features parameter.        
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with
            two keys: 'train_scores' and 'val_scores'. The values to this dictionary are also dictionaries, with as keys the name
            of the regression score and as values its numerical value.
    """
    num_occurrences = np.array(num_occurrences)
        
    print("Training on {} samples with {} features".format(*features.shape))
        
    results = []
    
    folds_generator = _folds_generator(features, num_occurrences, cv_n_splits, 'regression', verbose)
    for features_train, features_val, num_occurrences_train, num_occurrences_val in folds_generator:
        
        model = DummyRegressor("mean")
        model.fit(features_train, num_occurrences_train)

        current_fold_results = {}
        
        for key, X, y in [('train_scores', features_train, num_occurrences_train),
                          ('val_scores'  , features_val  , num_occurrences_val)]:
            
            preds = model.predict(X)
            
            current_fold_results[key] = _scores_regression(y, preds)
            
        results.append(current_fold_results)
            
    return results


@utils.cache_to_file_pickle("cross_validation-linear_regression_cv", ignore_kwargs = ['features', 'verbose'])
def linear_regression_cv(features, num_occurrences, features_cols_titles, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold.
    The regressor used by this function is a least squares regressor.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        num_occurrences::[np.array]
            Array containing the training outcomes corresponding to samples in features.
        features_cols_titles::[list | tuple]
            List of strings containing the name assigned to each column in features parameter.        
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with
            two keys: 'train_scores' and 'val_scores'. The values to this dictionary are also dictionaries, with as keys the name
            of the regression score and as values its numerical value.
    """

    def add_intercept(features, features_cols_titles):
        """
        Function adding a column filled with ones to features parameter and updating features_cols_titles accordingly.

        Params:
            features::[np.array | scipy.sparse.matrix]
                Matrix containing all the training samples.
            features_cols_titles::[list | tuple]
                List of strings containing the name assigned to each column in features parameter.        
           
        Returns:
            features::[np.array | scipy.sparse.matrix]
                Same matrix as input, with a column of ones added to its left. 
            features_cols_titles::[list | tuple]
                Same list as input, with a 'intercept' added as the first element. 
        """
        n_samples, n_features = features.shape

        features = scipy.sparse.hstack([np.ones((n_samples, 1), dtype = 'uint8'), features]).tocsr()
        features_cols_titles = ['intercept'] + features_cols_titles

        return features, features_cols_titles
    
    
    def compute_pvalues(X, y, coefs):
        """
        Function computing the P-value of the coefficients learnt by least squares regression.

        Params:
            X::[np.array | scipy.sparse.matrix]
                Matrix containing the samples used to train the least squares regressor.
            X_val::[np.array | scipy.sparse.matrix]
                Array containing the outcomes used to train the least squares regressor.
            coefs::[np.array]
                Coefficients learnt by the least squares regressor.
           
        Returns:
            p_values::[np.array]
                P-values associated to each coefficient learnt by least squares on this training set.
        """
        preds = X.dot(coefs)
        
        df = len(preds) - len(coefs)
        
        adjusted_mse = np.sum((y - preds)**2) / df
        
        var_coefs = adjusted_mse * scipy.sparse.linalg.inv(X.T.dot(X)).diagonal()
                
        # Compute t-statistics, avoiding warnings due to numerical errors.
        t_statistic_coefs = [coef / np.sqrt(var) if var > 0 else float('inf') for coef, var in zip(coefs, var_coefs)]

        p_values = [2 * (1 - scipy.stats.t.cdf(np.abs(t_statistic), df)) for t_statistic in t_statistic_coefs]
        
        return p_values
    
    num_occurrences = np.array(num_occurrences)
    
    features, features_cols_titles = add_intercept(features, features_cols_titles) 
    
    print("Training on {} samples with {} features".format(*features.shape))
        
    results = []
    
    folds_generator = _folds_generator(features, num_occurrences, cv_n_splits, 'regression', verbose)
    for features_train, features_val, num_occurrences_train, num_occurrences_val in folds_generator:
        
        coefs = scipy.sparse.linalg.lsqr(features_train, num_occurrences_train)[0]          
                
        current_fold_results = {'coefs': coefs, 
                                'pvalues': compute_pvalues(features_train, num_occurrences_train, coefs)}
        
        for key, X, y in [('train_scores', features_train, num_occurrences_train),
                          ('val_scores'  , features_val  , num_occurrences_val)]:
            
            preds = X.dot(coefs)
            
            current_fold_results[key] = _scores_regression(y, preds)
            
            # For linear regression, add adjusted R2 score to allow comparison between models with less and less features.
            r2 = current_fold_results[key]['r2']
            adjusted_r2 = 1 - (1 - r2) * (len(num_occurrences_train) - 1) / (len(num_occurrences_train) - len(coefs) - 1)
            current_fold_results[key]['adjusted_r2'] = adjusted_r2
            
        results.append(current_fold_results)
            
    return results, features_cols_titles




@utils.cache_to_file_pickle("cross_validation-tree_regression_cv", ignore_kwargs = ['features', 'verbose'])
def tree_regression_cv(features, num_occurrences, post_pruning_alphas, max_depth, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold, for each
    value in post_pruning_alphas.
    The regressor used by this function is a decision tree regressor.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        num_occurrences::[np.array]
            Array containing the training outcomes corresponding to samples in features.
        post_pruning_alphas::[iterable]
            Values to use to post-prune the decision tree and compute the scores for at each fold.
        max_depth::[int]
            Maximum depth of unpruned decision tree.
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with as
            key the possible values of pruning parameter alpha which were used (for each fold, the tens of pruning alpha values
            were used to prune the same base tree and then compute its performance on the validation and train set). Each value
            of this dict then contains another dict, with two keys: 'train_scores' and 'val_scores'. The values to this 
            dictionary are also dictionaries, with as keys the name of the regression score and as values its numerical value.
    """
    num_occurrences = np.array(num_occurrences)
    
    print("Training on {} samples with {} features".format(*features.shape))
    
    results = []
    
    # Train on single fold, evaluate on all other folds for computational cost reasons.
    folds_generator = _folds_generator(features, num_occurrences, cv_n_splits, 'regression', verbose, 
                                       train_on_single_fold = True)
    for features_train, features_val, num_occurrences_train, num_occurrences_val in folds_generator:
        
        # Train unpruned model a single time, with a specified maximum depth but otherwise no pre-pruning.
        # At each branch, tree will always be allowed to choose best split. As such, it should be deterministic
        # when provided the same data, irrespectively of post-pruning parameter ccp_alpha.
        unpruned_model = DecisionTreeRegressor(max_depth = max_depth).fit(features_train, num_occurrences_train) 
        
        current_fold_results = {alpha: {} for alpha in post_pruning_alphas}
        
        for alpha in post_pruning_alphas:
            # Instead of retraining multiple times the same tree and then post-pruning each time, train a single time,
            # deepcopy the instance and prune it once per each post-pruning parameter and compute scores. 
            model = deepcopy(unpruned_model)
            model.set_params(ccp_alpha = alpha)
            model._prune_tree()
            
            for key, X, y in [('train_scores', features_train, num_occurrences_train),
                              ('val_scores'  , features_val  , num_occurrences_val)]:
            
                preds = model.predict(X)

                current_fold_results[alpha][key] = _scores_regression(y, preds)
            
        results.append(current_fold_results)
            
    return results


@utils.cache_to_file_pickle("cross_validation-baseline_classification_cv", ignore_kwargs = ['features', 'verbose'])
def baseline_classification_cv(features, labels, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold.
    The classifiers used by this function do one of the following:
        - always predict 0
        - always predict 1
        - predict 0 or 1 with equal probability
        - predict 0 or 1 with same probability as their probability of appearing in training labels.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        labels::[np.array]
            Array containing the training labels corresponding to samples in features.
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with
            two keys: 'train_scores' and 'val_scores'. The values to this dictionary are also dictionaries, with as keys the name
            of the classification score and as values its numerical value.
    """
    labels = np.array(labels)
        
    print("Training on {} samples with {} features".format(*features.shape))
        
    strategies = ['uniform', 'stratified', 'constant_0', 'constant_1']
    results = {strategy: [] for strategy in strategies}
    
    folds_generator = _folds_generator(features, labels, cv_n_splits, 'classification', verbose)
    for features_train, features_val, labels_train, labels_val in folds_generator:
                
        for strategy in strategies:
            current_fold_strategy_results = {}
            
            if strategy.startswith('constant'):
                model = DummyClassifier('constant', constant = int(strategy.split('_')[-1]))
            else:
                model = DummyClassifier(strategy)

            model.fit(features_train, labels_train)

            for key, X, y in [('train_scores', features_train, labels_train),
                              ('val_scores'  , features_val  , labels_val)]:

                preds = model.predict(X)

                current_fold_strategy_results[key] = _scores_classification(y, preds)

            results[strategy].append(current_fold_strategy_results)
            
    return results




@utils.cache_to_file_pickle("cross_validation-linear_svm_classification_cv", ignore_kwargs = ['features', 'verbose'])
def linear_svm_classification_cv(features, labels, balanced_class_weight, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold.
    The classifier used by this function is Linear Support Vector Machine.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        labels::[np.array]
            Array containing the training labels corresponding to samples in features.
        balanced_class_weight::[bool]
            Wether to apply different weights in the SVM loss function such that equal importance is given to both classes.        
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with
            two keys: 'train_scores' and 'val_scores'. The values to this dictionary are also dictionaries, with as keys the name
            of the classification score and as values its numerical value.
    """    
    labels = np.array(labels)
    
    print("Training on {} samples with {} features".format(*features.shape))
    print(f"Positive samples: {np.sum(labels == 1)}, negative samples: {np.sum(labels == 0)}")

    results = []
    
    folds_generator = _folds_generator(features, labels, cv_n_splits, 'classification', verbose)
    for features_train, features_val, labels_train, labels_val in folds_generator:
        
        model = LinearSVC(dual = False, max_iter = 10000, class_weight = 'balanced' if balanced_class_weight else None)
        model.fit(features_train, labels_train) 
                
        current_fold_results = {}
        
        for key, X, y in [('train_scores', features_train, labels_train),
                          ('val_scores'  , features_val  , labels_val)]:
            
            preds = model.predict(X)
            
            current_fold_results[key] = _scores_classification(y, preds)
            
            # For support vector machines, add sparsity to allow comparison between models with less and less features.
            current_fold_results[key]['sparsity'] = np.mean(np.abs(model.coef_) < 1e-8)
            
        results.append(current_fold_results)
            
    return results


@utils.cache_to_file_pickle("cross_validation-tree_classification_cv", ignore_kwargs = ['features', 'verbose'])
def tree_classification_cv(features, labels, post_pruning_alphas, max_depth, cv_n_splits, verbose = False):
    """
    Function performing a cross-validation on the training data and returning the scores for each fold, for each
    value in post_pruning_alphas.
    The classifier used by this function is a decision tree classifier.
    
    Params:
        features::[np.array | scipy.sparse.matrix]
            Matrix containing all the training samples.
        labels::[np.array]
            Array containing the training labels corresponding to samples in features.
        post_pruning_alphas::[iterable]
            Values to use to post-prune the decision tree and compute the scores for at each fold.
        max_depth::[int]
            Maximum depth of unpruned decision tree.
        cv_n_splits::[int]
            Number of folds to split the training data into and perform cross-validation over.
        verbose::[bool]
            Wether to print information about the current fold each time it is yielded.
    
    Returns:
        results::[list[dict]]
            List of dictionaries. Each element of the list contains the results of one fold in the form of a dictionary with as
            key the possible values of pruning parameter alpha which were used (for each fold, the tens of pruning alpha values
            were used to prune the same base tree and then compute its performance on the validation and train set). Each value
            of this dict then contains another dict, with two keys: 'train_scores' and 'val_scores'. The values to this 
            dictionary are also dictionaries, with as keys the name of the classification score and as values its numerical value.
    """
    labels = np.array(labels)
    
    print("Training on {} samples with {} features".format(*features.shape))
    print(f"Positive samples: {np.sum(labels == 1)}, negative samples: {np.sum(labels == 0)}")
        
    results = []
    
    # Train on single fold, evaluate on all other folds for computational cost reasons.
    folds_generator = _folds_generator(features, labels, cv_n_splits, 'classification', verbose, 
                                       train_on_single_fold = True)
    for features_train, features_val, labels_train, labels_val in folds_generator:
        
        # Train unpruned model a single time, with a specified maximum depth but otherwise no pre-pruning.
        # At each branch, tree will always be allowed to choose best split. As such, it should be deterministic
        # when provided the same data, irrespectively of post-pruning parameter ccp_alpha.
        unpruned_model = DecisionTreeClassifier(max_depth = max_depth).fit(features_train, labels_train) 
        
        current_fold_results = {alpha: {} for alpha in post_pruning_alphas}
        
        for alpha in post_pruning_alphas:
            # Instead of retraining multiple times the same tree and then post-pruning each time, train a single time,
            # deepcopy the instance and prune it once per each post-pruning parameter and compute scores. 
            model = deepcopy(unpruned_model)
            model.set_params(ccp_alpha = alpha)
            model._prune_tree()
            
            for key, X, y in [('train_scores', features_train, labels_train),
                              ('val_scores'  , features_val  , labels_val)]:
            
                preds = model.predict(X)

                current_fold_results[alpha][key] = _scores_classification(y, preds)
            
        results.append(current_fold_results)
            
    return results


@utils.cache_to_file_pickle("cross_validation-get_pruning_alphas_impurities", ignore_kwargs = ['X_train'])
def get_pruning_alphas_impurities(X_train, y_train, problem_type, downsampling_factor, max_depth):
    """
    Wrapper around sklearn.tree.DecisionTree(Regressor | Classifier).cost_complexity_pruning_path which allows to downsample
    the dataset before calling the function.
    
    Params:
        X_train::[np.array | scipy.sparse.matrix]
            Matrix containing the training samples to use for the current fold.
        y_train::[np.array]
            Matrix containing the training outcomes to use for the current fold.
        problem_type::[str]
            'regression' if this function is used as folds generator for a regression problem and 'classification'
            if it is used for a classification problem.
        downsampling_factor::[float]
            Amount by which to downsample the training data (in a stratified manner) before calling the cost_complexity_pruning_path
            method.
        max_depth::[int]
            Maximum depth of unpruned decision tree.
        
    Returns:
        alphas::[np.array]
            List of alphas outputed by cost_complexity_pruning_path method. Correspond to the values of ccp_alpha parameter of
            decision tree which cause leaves to join. 
        impurities::[np.array]
            List of impurities outputed by cost_complexity_pruning_path method. Correspond to total leaves impurity after leaves are
            joined with corresponding value in alphas output.
    """
    y_train = np.array(y_train)
    
    if problem_type == 'regression':
        tree = DecisionTreeRegressor(max_depth = max_depth)
        stratifier = utils.get_percentile_stratifier(y_train)
    
    elif problem_type == 'classification':
        tree = DecisionTreeClassifier(max_depth = max_depth)
        stratifier = y_train

    else:
        raise ValueError("Parameter 'problem_type' must be either 'regression' or 'classification'.")
        
    
    if downsampling_factor > 1:
        X_train, _, y_train, _ = train_test_split(X_train, y_train, 
                                                  train_size = 1 / downsampling_factor, 
                                                  shuffle = True, 
                                                  stratify = stratifier)
        
    path = tree.cost_complexity_pruning_path(X_train, y_train)
    return path.ccp_alphas, path.impurities